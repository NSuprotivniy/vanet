{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nsuprotivniy/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "import pandas as pd\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.optimizers import Adam, RMSprop, SGD\n",
    "import sys, re, glob\n",
    "import numpy as np\n",
    "from keras.layers import Input, Dense, Flatten\n",
    "import sys, re, glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from lxml import etree\n",
    "from sklearn.model_selection import StratifiedKFold "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path, rng):\n",
    "    routes = list()\n",
    "    flows = list()\n",
    "\n",
    "    for i in range(rng):\n",
    "\n",
    "        for routes_file in glob.glob('{}/output_{}/*.routes'.format(path, i)):\n",
    "\n",
    "            info = re.findall('(\\w+)/output_(\\d+)', routes_file)\n",
    "\n",
    "            handle = open(routes_file, 'r')\n",
    "            data = handle.read()\n",
    "            handle.close()\n",
    "\n",
    "            nodes = re.split('\\n\\n', data)\n",
    "            nodes.pop()\n",
    "\n",
    "            for node in nodes:\n",
    "\n",
    "                header = re.findall('Node:\\s+(\\d+)\\s+Time:\\s+(\\d+)', node)\n",
    "                lines = re.findall('(\\d{1,3}(?:\\.\\d{1,3}){3})\\s+(\\d{1,3}(?:\\.\\d{1,3}){3})\\s+(\\d{1,3}(?:\\.\\d{1,3}){3})\\s+(\\w+)\\s+(-?\\d+\\.\\d+)\\s+(\\d+)', node)\n",
    "\n",
    "                for line in lines:\n",
    "                    l = list(line)\n",
    "                    l[4] = float(line[4])\n",
    "                    l[5] = int(line[5])\n",
    "                    routes.append(header[0] + tuple(l) + info[0])\n",
    "\n",
    "        for flowmon_file in glob.glob('{}/output_{}/*.flowmon'.format(path, i)):\n",
    " \n",
    "            info = re.findall('(\\w+)/output_(\\d+)', flowmon_file)\n",
    "\n",
    "            with open(flowmon_file) as fobj:\n",
    "                xml = fobj.read()\n",
    "    \n",
    "            root = etree.fromstring(xml)\n",
    "\n",
    "\n",
    "            for flow in root.xpath('/FlowMonitor/FlowStats/Flow'):\n",
    "\n",
    "                attributes = list()\n",
    "\n",
    "                for attrib in flow.attrib:\n",
    "\n",
    "                    attr = flow.attrib[attrib]\n",
    "                    if 'ns' in attr:\n",
    "                        attr = re.findall('(\\d+)', attr)[0]\n",
    "\n",
    "                    attributes.append(int(attr))\n",
    "\n",
    "                flows.append(tuple(attributes) + info[0])\n",
    "\n",
    "    routes_table = pd.DataFrame(routes, columns=['Node', 'Time', 'Destination', 'Gateway', 'Interface', 'Flag', 'Expire', 'Hops', 'Type', 'Test'])\n",
    "\n",
    "    flag_agg = routes_table.groupby(['Type', 'Test', 'Time', 'Flag']).agg({'Flag' : ['count']})\n",
    "    hops_agg = routes_table.groupby(['Type', 'Test', 'Node', 'Destination']).agg({'Hops' : ['min', 'max', 'mean', 'median', 'prod', 'sum', 'std', 'var']})\n",
    "    \n",
    "    unstack_flag_agg = flag_agg.unstack(['Time','Flag']).fillna(0)\n",
    "    unstack_hops_agg = hops_agg.unstack(['Node', 'Destination']).fillna(0)\n",
    "    \n",
    " \n",
    "    flows_table = pd.DataFrame(flows, columns=['flowId', 'timeFirstTxPacket', 'timeFirstRxPacket', 'timeLastTxPacket', 'timeLastRxPacket', 'delaySum', 'jitterSum', 'lastDelay', 'txBytes', 'rxBytes', 'txPackets', 'rxPackets', 'lostPackets', 'timesForwarded', 'Type', 'Test'])\n",
    "\n",
    "    lost_agg = flows_table.groupby(['Type', 'Test']).agg({'lostPackets' : ['sum', 'mean']})\n",
    "    forwarded_agg = flows_table.groupby(['Type', 'Test']).agg({'timesForwarded' : ['sum', 'max', 'mean', 'var']})\n",
    "    \n",
    "    return lost_agg.join(forwarded_agg).join(unstack_flag_agg).join(unstack_hops_agg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nsuprotivniy/anaconda3/lib/python3.6/site-packages/pandas/core/reshape/merge.py:558: UserWarning: merging between different levels can give an unintended result (2 levels on the left, 4 on the right)\n",
      "  warnings.warn(msg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "malicious = load_data(\"../data/malicious\", 100)\n",
    "normal = load_data(\"../data/normal\", 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.concatenate((malicious, normal))\n",
    "y = np.concatenate((np.ones((100, 1)), np.zeros((100, 1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes = np.array(range(len(y)))\n",
    "np.random.shuffle(indexes)\n",
    "indexes\n",
    "X = X[indexes]\n",
    "y = y[indexes]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAN:\n",
    "    def __init__(self, shape):\n",
    "        self.SHAPE = shape\n",
    "        self.OPTIMIZER = Adam()\n",
    "        self.compile_models()\n",
    "        \n",
    "    def __generator(self):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(2048, input_shape=(1000,)))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Dense(self.SHAPE, activation='sigmoid'))\n",
    "        return model\n",
    "    \n",
    "    def __discriminator(self):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(10, input_shape=(self.SHAPE,)))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "        return model\n",
    "    \n",
    "    def __stacked(self, generator, discriminator):\n",
    "        discriminator.trainable = False\n",
    "        model = Sequential()\n",
    "        model.add(generator)\n",
    "        model.add(discriminator)\n",
    "        return model\n",
    "    \n",
    "    def compile_models(self):\n",
    "        self.generator = self.__generator()\n",
    "        self.discriminator = self.__discriminator()\n",
    "        self.stacked = self.__stacked(self.generator, self.discriminator)\n",
    "        \n",
    "        self.generator.compile(loss='binary_crossentropy', optimizer=self.OPTIMIZER)\n",
    "        self.discriminator.compile(loss='binary_crossentropy', optimizer=self.OPTIMIZER, metrics=['accuracy'] )\n",
    "        self.stacked.compile(loss='binary_crossentropy', optimizer=self.OPTIMIZER)\n",
    "        \n",
    "    \n",
    "    def train(self,X, y, epochs=200, batch = 100, debug=False):\n",
    "        for cnt in range(epochs):\n",
    "\n",
    "            ## train discriminator\n",
    "            random_index =  np.random.randint(0, len(y) - batch)\n",
    "            X_batch = X[random_index : random_index + batch]\n",
    "            y_batch = y[random_index : random_index + batch]\n",
    "\n",
    "            gen_noise = np.random.normal(0, 1, (batch,1000))\n",
    "            syntetic = self.generator.predict(gen_noise)\n",
    "                \n",
    "            x_combined_batch = np.concatenate((X_batch, syntetic))\n",
    "            y_combined_batch = np.concatenate((y_batch, np.zeros((batch, 1))))\n",
    "\n",
    "            d_loss = self.discriminator.train_on_batch(x_combined_batch, y_combined_batch)\n",
    "\n",
    "            # train generator\n",
    "            noise = np.random.normal(0, 1, (batch,1000))\n",
    "            y_mislabled = np.ones((batch, 1))\n",
    "            g_loss = self.stacked.train_on_batch(noise, y_mislabled)\n",
    "            if debug:\n",
    "                print ('epoch: %d, [Discriminator :: d_loss: %f], [ Generator :: loss: %f]' % (cnt, d_loss[0], g_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, [Discriminator :: d_loss: 5.350269], [ Generator :: loss: 0.633688]\n",
      "epoch: 1, [Discriminator :: d_loss: 5.427353], [ Generator :: loss: 0.541678]\n",
      "epoch: 2, [Discriminator :: d_loss: 5.561776], [ Generator :: loss: 0.444455]\n",
      "epoch: 3, [Discriminator :: d_loss: 5.743180], [ Generator :: loss: 0.328258]\n",
      "epoch: 4, [Discriminator :: d_loss: 5.935463], [ Generator :: loss: 0.231793]\n",
      "epoch: 5, [Discriminator :: d_loss: 6.276007], [ Generator :: loss: 0.163799]\n",
      "epoch: 6, [Discriminator :: d_loss: 6.457485], [ Generator :: loss: 0.108152]\n",
      "epoch: 7, [Discriminator :: d_loss: 6.711811], [ Generator :: loss: 0.077761]\n",
      "epoch: 8, [Discriminator :: d_loss: 7.189558], [ Generator :: loss: 0.057855]\n",
      "epoch: 9, [Discriminator :: d_loss: 7.412441], [ Generator :: loss: 0.055682]\n",
      "epoch: 10, [Discriminator :: d_loss: 7.796431], [ Generator :: loss: 0.039348]\n",
      "epoch: 11, [Discriminator :: d_loss: 8.177227], [ Generator :: loss: 0.032155]\n",
      "epoch: 12, [Discriminator :: d_loss: 8.508833], [ Generator :: loss: 0.030578]\n",
      "epoch: 13, [Discriminator :: d_loss: 8.685696], [ Generator :: loss: 0.033419]\n",
      "epoch: 14, [Discriminator :: d_loss: 9.062062], [ Generator :: loss: 0.026814]\n",
      "epoch: 15, [Discriminator :: d_loss: 9.465035], [ Generator :: loss: 0.021629]\n",
      "epoch: 16, [Discriminator :: d_loss: 9.340303], [ Generator :: loss: 0.012090]\n",
      "epoch: 17, [Discriminator :: d_loss: 9.726195], [ Generator :: loss: 0.012494]\n",
      "epoch: 18, [Discriminator :: d_loss: 9.943409], [ Generator :: loss: 0.034661]\n",
      "epoch: 19, [Discriminator :: d_loss: 9.749093], [ Generator :: loss: 0.006700]\n",
      "epoch: 20, [Discriminator :: d_loss: 10.233676], [ Generator :: loss: 0.006105]\n",
      "epoch: 21, [Discriminator :: d_loss: 10.248288], [ Generator :: loss: 0.020975]\n",
      "epoch: 22, [Discriminator :: d_loss: 10.624853], [ Generator :: loss: 0.011594]\n",
      "epoch: 23, [Discriminator :: d_loss: 10.664625], [ Generator :: loss: 0.002999]\n",
      "epoch: 24, [Discriminator :: d_loss: 10.994213], [ Generator :: loss: 0.019495]\n",
      "epoch: 25, [Discriminator :: d_loss: 10.668354], [ Generator :: loss: 0.022375]\n",
      "epoch: 26, [Discriminator :: d_loss: 10.903997], [ Generator :: loss: 0.006028]\n",
      "epoch: 27, [Discriminator :: d_loss: 10.894206], [ Generator :: loss: 0.005044]\n",
      "epoch: 28, [Discriminator :: d_loss: 10.972809], [ Generator :: loss: 0.007986]\n",
      "epoch: 29, [Discriminator :: d_loss: 10.840196], [ Generator :: loss: 0.014436]\n",
      "epoch: 30, [Discriminator :: d_loss: 11.106613], [ Generator :: loss: 0.028955]\n",
      "epoch: 31, [Discriminator :: d_loss: 11.279021], [ Generator :: loss: 0.049629]\n",
      "epoch: 32, [Discriminator :: d_loss: 11.234197], [ Generator :: loss: 0.012179]\n",
      "epoch: 33, [Discriminator :: d_loss: 11.133051], [ Generator :: loss: 0.052574]\n",
      "epoch: 34, [Discriminator :: d_loss: 11.280389], [ Generator :: loss: 0.025401]\n",
      "epoch: 35, [Discriminator :: d_loss: 11.256876], [ Generator :: loss: 0.017586]\n",
      "epoch: 36, [Discriminator :: d_loss: 11.395598], [ Generator :: loss: 0.004276]\n",
      "epoch: 37, [Discriminator :: d_loss: 11.397702], [ Generator :: loss: 0.060561]\n",
      "epoch: 38, [Discriminator :: d_loss: 11.369017], [ Generator :: loss: 0.038680]\n",
      "epoch: 39, [Discriminator :: d_loss: 11.582837], [ Generator :: loss: 0.029938]\n",
      "epoch: 40, [Discriminator :: d_loss: 11.470284], [ Generator :: loss: 0.008096]\n",
      "epoch: 41, [Discriminator :: d_loss: 11.498742], [ Generator :: loss: 0.068560]\n",
      "epoch: 42, [Discriminator :: d_loss: 11.654643], [ Generator :: loss: 0.016152]\n",
      "epoch: 43, [Discriminator :: d_loss: 11.708163], [ Generator :: loss: 0.002898]\n",
      "epoch: 44, [Discriminator :: d_loss: 11.537568], [ Generator :: loss: 0.014827]\n",
      "epoch: 45, [Discriminator :: d_loss: 11.398588], [ Generator :: loss: 0.041277]\n",
      "epoch: 46, [Discriminator :: d_loss: 11.656528], [ Generator :: loss: 0.052216]\n",
      "epoch: 47, [Discriminator :: d_loss: 11.710424], [ Generator :: loss: 0.105105]\n",
      "epoch: 48, [Discriminator :: d_loss: 11.769698], [ Generator :: loss: 0.010731]\n",
      "epoch: 49, [Discriminator :: d_loss: 11.697724], [ Generator :: loss: 0.043228]\n",
      "epoch: 50, [Discriminator :: d_loss: 12.000806], [ Generator :: loss: 0.002073]\n",
      "epoch: 51, [Discriminator :: d_loss: 11.737113], [ Generator :: loss: 0.016438]\n",
      "epoch: 52, [Discriminator :: d_loss: 11.779901], [ Generator :: loss: 0.082028]\n",
      "epoch: 53, [Discriminator :: d_loss: 11.587089], [ Generator :: loss: 0.078150]\n",
      "epoch: 54, [Discriminator :: d_loss: 11.777677], [ Generator :: loss: 0.024196]\n",
      "epoch: 55, [Discriminator :: d_loss: 11.682141], [ Generator :: loss: 0.064336]\n",
      "epoch: 56, [Discriminator :: d_loss: 11.851978], [ Generator :: loss: 0.044751]\n",
      "epoch: 57, [Discriminator :: d_loss: 11.905747], [ Generator :: loss: 0.061512]\n",
      "epoch: 58, [Discriminator :: d_loss: 12.062084], [ Generator :: loss: 0.056876]\n",
      "epoch: 59, [Discriminator :: d_loss: 12.047608], [ Generator :: loss: 0.009215]\n",
      "epoch: 60, [Discriminator :: d_loss: 12.111629], [ Generator :: loss: 0.049645]\n",
      "epoch: 61, [Discriminator :: d_loss: 11.936095], [ Generator :: loss: 0.045596]\n",
      "epoch: 62, [Discriminator :: d_loss: 11.878495], [ Generator :: loss: 0.098206]\n",
      "epoch: 63, [Discriminator :: d_loss: 12.099321], [ Generator :: loss: 0.007557]\n",
      "epoch: 64, [Discriminator :: d_loss: 12.140964], [ Generator :: loss: 0.019672]\n",
      "epoch: 65, [Discriminator :: d_loss: 12.191127], [ Generator :: loss: 0.016592]\n",
      "epoch: 66, [Discriminator :: d_loss: 12.088656], [ Generator :: loss: 0.102775]\n",
      "epoch: 67, [Discriminator :: d_loss: 12.072587], [ Generator :: loss: 0.008106]\n",
      "epoch: 68, [Discriminator :: d_loss: 12.154737], [ Generator :: loss: 0.011788]\n",
      "epoch: 69, [Discriminator :: d_loss: 12.164584], [ Generator :: loss: 0.128925]\n",
      "epoch: 70, [Discriminator :: d_loss: 12.298434], [ Generator :: loss: 0.085091]\n",
      "epoch: 71, [Discriminator :: d_loss: 12.229464], [ Generator :: loss: 0.035347]\n",
      "epoch: 72, [Discriminator :: d_loss: 12.009590], [ Generator :: loss: 0.064014]\n",
      "epoch: 73, [Discriminator :: d_loss: 12.191440], [ Generator :: loss: 0.003182]\n",
      "epoch: 74, [Discriminator :: d_loss: 12.175316], [ Generator :: loss: 0.093221]\n",
      "epoch: 75, [Discriminator :: d_loss: 12.193721], [ Generator :: loss: 0.003662]\n",
      "epoch: 76, [Discriminator :: d_loss: 12.118829], [ Generator :: loss: 0.118997]\n",
      "epoch: 77, [Discriminator :: d_loss: 12.071057], [ Generator :: loss: 0.126405]\n",
      "epoch: 78, [Discriminator :: d_loss: 12.295384], [ Generator :: loss: 0.139474]\n",
      "epoch: 79, [Discriminator :: d_loss: 12.261059], [ Generator :: loss: 0.187451]\n",
      "epoch: 80, [Discriminator :: d_loss: 12.136431], [ Generator :: loss: 0.002455]\n",
      "epoch: 81, [Discriminator :: d_loss: 12.390404], [ Generator :: loss: 0.033587]\n",
      "epoch: 82, [Discriminator :: d_loss: 12.393158], [ Generator :: loss: 0.033300]\n",
      "epoch: 83, [Discriminator :: d_loss: 12.380320], [ Generator :: loss: 0.007224]\n",
      "epoch: 84, [Discriminator :: d_loss: 12.244158], [ Generator :: loss: 0.032540]\n",
      "epoch: 85, [Discriminator :: d_loss: 12.227874], [ Generator :: loss: 0.017909]\n",
      "epoch: 86, [Discriminator :: d_loss: 12.538939], [ Generator :: loss: 0.011329]\n",
      "epoch: 87, [Discriminator :: d_loss: 12.300542], [ Generator :: loss: 0.043387]\n",
      "epoch: 88, [Discriminator :: d_loss: 12.374595], [ Generator :: loss: 0.037144]\n",
      "epoch: 89, [Discriminator :: d_loss: 12.370249], [ Generator :: loss: 0.127440]\n",
      "epoch: 90, [Discriminator :: d_loss: 12.267076], [ Generator :: loss: 0.005356]\n",
      "epoch: 91, [Discriminator :: d_loss: 12.294825], [ Generator :: loss: 0.081236]\n",
      "epoch: 92, [Discriminator :: d_loss: 12.559237], [ Generator :: loss: 0.061795]\n",
      "epoch: 93, [Discriminator :: d_loss: 12.475826], [ Generator :: loss: 0.000073]\n",
      "epoch: 94, [Discriminator :: d_loss: 12.574035], [ Generator :: loss: 0.025620]\n",
      "epoch: 95, [Discriminator :: d_loss: 12.535358], [ Generator :: loss: 0.076324]\n",
      "epoch: 96, [Discriminator :: d_loss: 12.595630], [ Generator :: loss: 0.001373]\n",
      "epoch: 97, [Discriminator :: d_loss: 12.461919], [ Generator :: loss: 0.000992]\n",
      "epoch: 98, [Discriminator :: d_loss: 12.431663], [ Generator :: loss: 0.000180]\n",
      "epoch: 99, [Discriminator :: d_loss: 12.411938], [ Generator :: loss: 0.000233]\n"
     ]
    }
   ],
   "source": [
    "gan = GAN(malicious.shape[1])\n",
    "gan.train(X, y, debug=True, epochs=100, batch=199)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preidiction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = gan.discriminator.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.375"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(y == (predict > 0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 96   4]\n",
      " [100   0]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.48"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import metrics \n",
    "print(metrics.confusion_matrix(y, (predict > 0.5)))\n",
    " \n",
    "metrics.accuracy_score(y, (predict > 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.25\n",
      "0.55\n",
      "0.7\n",
      "0.5\n",
      "0.45\n",
      "0.4\n",
      "0.4\n",
      "0.65\n",
      "0.55\n",
      "0.25\n",
      "0.47% (+/- 0.14%)\n"
     ]
    }
   ],
   "source": [
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=17) \n",
    "cvscores = [] \n",
    "for train, test in kfold.split(X, y): \n",
    "    gan = GAN(malicious.shape[1]) \n",
    "    gan.train(X[train], y[train], epochs=100, batch=100, debug=False) \n",
    "    predict = gan.discriminator.predict(X[test]) \n",
    "    acc = metrics.accuracy_score(y[test], (predict > 0.5)) \n",
    "    cvscores.append(acc) \n",
    "    print(acc) \n",
    "\n",
    "print(\"%.2f%% (+/- %.2f%%)\" % (np.mean(cvscores), np.std(cvscores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
