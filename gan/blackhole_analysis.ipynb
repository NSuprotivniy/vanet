{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys, os, pickle\n",
    "from keras.models import Sequential, model_from_json\n",
    "import pandas as pd\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.optimizers import Adam, RMSprop, SGD\n",
    "import sys, re, glob\n",
    "import numpy as np\n",
    "from keras.layers import Input, Dense, Flatten\n",
    "import sys, re, glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from lxml import etree\n",
    "from sklearn.model_selection import StratifiedKFold "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "proto_dict = {\n",
    "    'ns3::WifiMacHeader' : 'MAC',\n",
    "    'ns3::LlcSnapHeader' : 'LLC',\n",
    "    'ns3::ArpHeader' : 'ARP',\n",
    "    'ns3::Ipv4Header' : 'IPv4',\n",
    "    'ns3::UdpHeader' : 'UDP',\n",
    "    'ns3::aodv::TypeHeader' : 'AODV_Type',\n",
    "    'ns3::aodv::RrepHeader' : 'AODV_RREP',\n",
    "    'ns3::aodv::RreqHeader' : 'AODV_RREQ',\n",
    "    'ns3::aodv::RrepAckHeader' : 'AODV_RACK',\n",
    "    'ns3::aodv::RerrHeader' : 'AODV_RERR',\n",
    "    'ns3::Icmpv4Header' : 'ICMPv4_HEADER',\n",
    "    'ns3::Icmpv4TimeExceeded' : 'ICMPv4_TE',\n",
    "    'ns3::Icmpv4DestinationUnreachable' : 'ICMPv4_DU'\n",
    "}\n",
    "\n",
    "def parce_packets(input_file):\n",
    "    with open(input_file, 'r') as f:\n",
    "        data = f.read()\n",
    "    data = data.split('meta-info=\"')\n",
    "    meta_info = []\n",
    "    data.pop(0)\n",
    "    for b in data:\n",
    "        meta_info.append(b.split('\"')[0])\n",
    "    packets = []\n",
    "    for m in meta_info:\n",
    "        raw = {}\n",
    "        for proto in proto_dict:\n",
    "            if m.find(proto) != -1:\n",
    "                raw[proto_dict[proto]] = m.split(proto+' (')[1].split(') ns3')[0]\n",
    "        p = {}\n",
    "        if 'MAC' in raw and 'LLC' in raw:\n",
    "            p['MAC'] = {}\n",
    "            p['MAC']['src'] = raw['MAC'].split('SA=')[1].split(',')[0]\n",
    "            p['MAC']['dst'] = raw['MAC'].split('DA=')[1].split(',')[0]\n",
    "            p['MAC']['type'] = int(raw['LLC'].split('type ')[1], 16)\n",
    "        if 'ARP' in raw:\n",
    "            p['ARP'] = {}\n",
    "            if raw['ARP'].find('request') != -1:\n",
    "                p['ARP']['type'] = 'request'\n",
    "            else:\n",
    "                p['ARP']['type'] = 'reply'\n",
    "                p['ARP']['MAC_dst'] = raw['ARP'].split('dest mac: ')[1].split(' ')[0]\n",
    "            p['ARP']['MAC_src'] = raw['ARP'].split('source mac: ')[1].split(' ')[0]\n",
    "            p['ARP']['IP_src'] = raw['ARP'].split('source ipv4: ')[1].split(' ')[0]\n",
    "            p['ARP']['IP_dst'] = raw['ARP'].split('dest ipv4: ')[1]\n",
    "        if 'IPv4' in raw:\n",
    "            p['IPv4'] = {}\n",
    "            ipv4data = raw['IPv4'].split(' ')\n",
    "            p['IPv4']['src'] = ipv4data[-3]\n",
    "            p['IPv4']['dst'] = ipv4data[-1]\n",
    "            p['IPv4']['proto'] = int(raw['IPv4'].split('protocol ')[1].split(' ')[0])\n",
    "            p['IPv4']['ttl'] = int(raw['IPv4'].split('ttl ')[1].split(' ')[0])\n",
    "            p['IPv4']['length'] = int(raw['IPv4'].split('length: ')[1].split(' ')[0])\n",
    "        if 'UDP' in raw:\n",
    "            p['UDP'] = {}\n",
    "            p['UDP']['length'] = int(raw['UDP'].split('length: ')[1].split(' ')[0])\n",
    "        if 'AODV_Type' in raw:\n",
    "            p['AODV'] = {}\n",
    "            p['AODV']['type'] = raw['AODV_Type']\n",
    "        if 'AODV_RREP' in raw:\n",
    "            p['AODV']['RREP'] = {}\n",
    "            p['AODV']['RREP']['dst'] = raw['AODV_RREP'].split('destination: ipv4 ')[1].split(' ')[0]\n",
    "            p['AODV']['RREP']['src'] = raw['AODV_RREP'].split('source ipv4 ')[1].split(' ')[0]\n",
    "            p['AODV']['RREP']['SN'] = int(raw['AODV_RREP'].split('sequence number ')[1].split(' ')[0])\n",
    "            p['AODV']['RREP']['lifetime'] = int(raw['AODV_RREP'].split('lifetime ')[1].split(' ')[0])\n",
    "            p['AODV']['RREP']['acknowledgment'] = raw['AODV_RREP'].split('acknowledgment ')[1].split(' ')[0]\n",
    "            p['AODV']['RREP']['flag'] = raw['AODV_RREP'].split('flag ')[1]\n",
    "        if 'AODV_RREQ' in raw:\n",
    "            p['AODV']['RREQ'] = {}\n",
    "            p['AODV']['RREQ']['ID'] = int(raw['AODV_RREQ'].split('ID ')[1].split(' ')[0])\n",
    "            p['AODV']['RREQ']['dst'] = raw['AODV_RREQ'].split('destination: ipv4 ')[1].split(' ')[0]\n",
    "            p['AODV']['RREQ']['src'] = raw['AODV_RREQ'].split('source: ipv4 ')[1].split(' ')[0]\n",
    "            p['AODV']['RREQ']['SN'] = int(raw['AODV_RREQ'].split('sequence number ')[1].split(' ')[0])\n",
    "            p['AODV']['RREQ']['flags'] = raw['AODV_RREQ'].split('flags: ')[1]\n",
    "        if 'AODV_RACK' in raw:\n",
    "            pass # seems to be empty\n",
    "        if 'AODV_RERR' in raw:\n",
    "            p['AODV']['RERR'] = raw['AODV_RERR']\n",
    "        if 'ICMPv4_HEADER' in raw:\n",
    "            p['ICMPv4'] = {}\n",
    "            p['ICMPv4']['type'] = int(raw['ICMPv4_HEADER'].split('type=')[1].split(',')[0])\n",
    "            p['ICMPv4']['code'] = int(raw['ICMPv4_HEADER'].split('code=')[1])\n",
    "        if 'ICMPv4_TE' in raw:\n",
    "            p['ICMPv4']['data'] = raw['ICMPv4_TE']\n",
    "        if 'ICMPv4_DU' in raw:\n",
    "            p['ICMPv4']['data'] = raw['ICMPv4_DU']\n",
    "\n",
    "        if len(p) > 0:\n",
    "            packets.append(p)\n",
    "    return packets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mac_to_id(mac):\n",
    "    return int(mac.split(':')[-1], 16) - 1\n",
    "\n",
    "def ip_to_id(ip):\n",
    "    return int(ip.split('.')[-1]) - 1\n",
    "\n",
    "def entropy(arr):\n",
    "    s = arr.sum()\n",
    "    arr = arr / s\n",
    "    arr = arr * np.log(arr) / np.log(2)\n",
    "    return -(arr.sum() / np.log(len(arr)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path, rng):\n",
    "    routes = list()\n",
    "    flows = list()\n",
    "    packets = list()\n",
    "\n",
    "    for i in range(rng):\n",
    "\n",
    "        for routes_file in glob.glob('{}/output_{}/*.routes'.format(path, i)):\n",
    "\n",
    "            info = re.findall('(\\w+)/output_(\\d+)', routes_file)\n",
    "\n",
    "            handle = open(routes_file, 'r')\n",
    "            data = handle.read()\n",
    "            handle.close()\n",
    "\n",
    "            nodes = re.split('\\n\\n', data)\n",
    "            nodes.pop()\n",
    "\n",
    "            for node in nodes:\n",
    "\n",
    "                header = re.findall('Node:\\s+(\\d+)\\s+Time:\\s+(\\d+)', node)\n",
    "                lines = re.findall('(\\d{1,3}(?:\\.\\d{1,3}){3})\\s+(\\d{1,3}(?:\\.\\d{1,3}){3})\\s+(\\d{1,3}(?:\\.\\d{1,3}){3})\\s+(\\w+)\\s+(-?\\d+\\.\\d+)\\s+(\\d+)', node)\n",
    "\n",
    "                for line in lines:\n",
    "                    l = list(line)\n",
    "                    l[4] = float(line[4])\n",
    "                    l[5] = int(line[5])\n",
    "                    routes.append(header[0] + tuple(l) + info[0])\n",
    "\n",
    "        for flowmon_file in glob.glob('{}/output_{}/*.flowmon'.format(path, i)):\n",
    "\n",
    "            info = re.findall('(\\w+)/output_(\\d+)', flowmon_file)\n",
    "\n",
    "            with open(flowmon_file) as fobj:\n",
    "                xml = fobj.read()\n",
    "\n",
    "            root = etree.fromstring(xml)\n",
    "\n",
    "\n",
    "            for flow in root.xpath('/FlowMonitor/FlowStats/Flow'):\n",
    "\n",
    "                attributes = list()\n",
    "\n",
    "                for attrib in flow.attrib:\n",
    "\n",
    "                    attr = flow.attrib[attrib]\n",
    "                    if 'ns' in attr:\n",
    "                        attr = re.findall('(\\d+)', attr)[0]\n",
    "\n",
    "                    attributes.append(int(attr))\n",
    "\n",
    "                flows.append(tuple(attributes) + info[0])\n",
    "\n",
    "        for packets_file in glob.glob('{}/output_{}/*.xml'.format(path, i)):\n",
    "\n",
    "            if \"routingtable-wireless.xml\" in packets_file:\n",
    "                continue\n",
    "\n",
    "            info = re.findall('(\\w+)/output_(\\d+)', packets_file)\n",
    "\n",
    "            packets_dict = parce_packets(packets_file)\n",
    "            \n",
    "            table_packets = []\n",
    "            for p in packets_dict:\n",
    "                if 'IPv4' in p:\n",
    "                    table_packets.append([mac_to_id(p['MAC']['src']), mac_to_id(p['MAC']['dst']), ip_to_id(p['IPv4']['src']), ip_to_id(p['IPv4']['dst'])])\n",
    "\n",
    "            table_packets = pd.DataFrame(table_packets, columns=['mac_src', 'mac_dst', 'ip_src', 'ip_dst'])\n",
    "            table_packets = table_packets[table_packets['ip_dst'] != 254]\n",
    "            table_packets['input'] = 0\n",
    "            table_packets['output'] = 0\n",
    "            input_cnt = table_packets.groupby('mac_src').agg({'input':'count'})\n",
    "            output_cnt = table_packets.groupby('mac_dst').agg({'output':'count'})\n",
    "            io_mac = input_cnt.join(output_cnt)\n",
    "            io_mac[\"diff\"] = io_mac['input'] - io_mac['output']\n",
    "            io_mac[\"normalized\"] = io_mac[\"diff\"] - io_mac[\"diff\"].min() + 1\n",
    "            \n",
    "            input = io_mac.agg({'input': [entropy, 'min', 'max', 'mean', 'var']}).values.flatten()\n",
    "            output = io_mac.agg({'output': [entropy, 'min', 'max', 'mean', 'var']}).values.flatten()\n",
    "            diff = io_mac.agg({'diff': ['min', 'max', 'mean', 'var']}).values.flatten()\n",
    "            normalized = io_mac.agg({'normalized': [entropy, 'min', 'max', 'mean', 'var']}).values.flatten()\n",
    "            src_count = table_packets[table_packets['mac_src'] == table_packets['ip_src']].count()[0]\n",
    "            dst_count = table_packets[table_packets['mac_dst'] == table_packets['ip_dst']].count()[0]\n",
    "\n",
    "            packets.append(np.concatenate((input, output, diff, normalized, [src_count, dst_count], info[0])))\n",
    "\n",
    "\n",
    "    routes_table = pd.DataFrame(routes, columns=['Node', 'Time', 'Destination', 'Gateway', 'Interface', 'Flag', 'Expire', 'Hops', 'Type', 'Test'])\n",
    "\n",
    "    #print(routes_table)\n",
    "    \n",
    "    flag_count = routes_table.groupby(['Type', 'Test', 'Time', 'Flag']).agg({'Flag' : ['count']})\n",
    "    flag_count = flag_count.reset_index(col_level=1)\n",
    "    flag_count.columns = flag_count.columns.droplevel()\n",
    "    flag_agg = flag_count.groupby(['Type', 'Test', 'Flag']).agg({'count': ['max', 'min', 'mean', 'var']}).unstack().reset_index()\n",
    "    flag_agg.columns = [col[0]+col[1]+col[2] for col in flag_agg.columns]\n",
    "    flag_agg = flag_agg.set_index(['Type', 'Test'])\n",
    "\n",
    "    hops_node_dest_agg = routes_table.groupby(['Type', 'Test', 'Node', 'Destination']).agg({'Hops' : ['min', 'max', 'mean']})\n",
    "    hops_node_agg = hops_node_dest_agg.reset_index().groupby([\"Type\", \"Test\", \"Node\"]).agg(['min', 'max', 'mean']).reset_index(col_level=1)\n",
    "    hops_node_agg.columns = hops_node_agg.columns.droplevel()\n",
    "    hops_agg = hops_node_agg.reset_index().groupby([\"Type\", \"Test\"]).agg(['min', 'max', 'mean']).reset_index(col_level=1)\n",
    "    hops_agg.columns = [col[0]+col[1]+col[2] for col in hops_agg.columns]\n",
    "    hops_agg = hops_agg.set_index(['Type', 'Test'])\n",
    "\n",
    "    flows_table = pd.DataFrame(flows, columns=['flowId', 'timeFirstTxPacket', 'timeFirstRxPacket', 'timeLastTxPacket', 'timeLastRxPacket', 'delaySum', 'jitterSum', 'lastDelay', 'txBytes', 'rxBytes', 'txPackets', 'rxPackets', 'lostPackets', 'timesForwarded', 'Type', 'Test'])\n",
    "\n",
    "    lost_agg = flows_table.groupby(['Type', 'Test']).agg({'lostPackets' : ['sum', 'mean']})\n",
    "    forwarded_agg = flows_table.groupby(['Type', 'Test']).agg({'timesForwarded' : ['sum', 'max', 'mean', 'var']})\n",
    "\n",
    "    packets_table = pd.DataFrame(packets, columns=[\"entopy_input\", \"min_input\", \"max_input\", \"mean_input\", \"var_input\", \"entopy_output\", \"min_output\", \"max_output\", \"mean_output\", \"var_output\", \"min_diff\", \"max_diff\", \"mean_diff\", \"var_diff\", \"entopy_normolized\", \"min_normolized\", \"max_normolized\", \"mean_normolized\", \"var_normolized\", \"src_count\", \"dst_count\", 'Type', 'Test'])\n",
    "    packets_table = packets_table.set_index(['Type', 'Test'])\n",
    "\n",
    "    return flag_agg.join(hops_agg).join(lost_agg).join(forwarded_agg).join(packets_table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\mrale\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\pandas\\core\\generic.py:3111: PerformanceWarning: dropping on a non-lexsorted multi-index without a level parameter may impact performance.\n",
      "  obj = obj._drop_axis(labels, axis, level=level, errors=errors)\n",
      "c:\\users\\mrale\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\pandas\\core\\reshape\\merge.py:544: UserWarning: merging between different levels can give an unintended result (1 levels on the left, 2 on the right)\n",
      "  warnings.warn(msg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "normal = load_data(\"../../../normal\", 100)\n",
    "blackhole = load_data(\"../../../blackhole\", 100)\n",
    "grayhole = load_data(\"../../../greyhole\", 100)\n",
    "wormhole = load_data(\"../../../wormhole\", 100)\n",
    "ddos = load_data(\"../../../ddos\", 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.concatenate((normal, blackhole, grayhole, ddos)).astype(np.float)\n",
    "y = np.concatenate((np.zeros((100, 1)), np.ones((300, 1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes = np.array(range(len(y)))\n",
    "np.random.shuffle(indexes)\n",
    "indexes\n",
    "X = X[indexes]\n",
    "y = y[indexes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[np.isnan(X)] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAN:\n",
    "    def __init__(self, shape):\n",
    "        self.SHAPE = shape\n",
    "        self.OPTIMIZER = Adam()\n",
    "        self.compile_models()\n",
    "        \n",
    "    def __generator(self):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(2048, input_shape=(1000,)))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dense(1024))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dense(512))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dense(256))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dense(self.SHAPE, activation='sigmoid'))\n",
    "        #model = Sequential()\n",
    "        #model.add(Dense(2048, input_shape=(1000,)))\n",
    "        #model.add(Dense(4096))\n",
    "        #model.add(Dense(4096))\n",
    "        #model.add(Dense(4096))\n",
    "        #model.add(Dense(4096))\n",
    "        #model.add(Dense(4096))\n",
    "        #model.add(Dense(4096))\n",
    "        #model.add(Dense(self.SHAPE, activation='sigmoid'))\n",
    "        return model\n",
    "    \n",
    "    def __discriminator(self):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(10, input_shape=(self.SHAPE,)))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Dense(256))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Dense(512))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Dense(1024))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "        #model = Sequential()\n",
    "        #model.add(Dense(10, input_shape=(self.SHAPE,)))\n",
    "        #model.add(Dense(4096))\n",
    "        #model.add(Dense(4096))\n",
    "        #model.add(Dense(4096))\n",
    "        #model.add(Dense(4096))\n",
    "        #model.add(Dense(4096))\n",
    "        #model.add(Dense(4096))\n",
    "        #model.add(Dense(1, activation='sigmoid'))\n",
    "        return model\n",
    "    \n",
    "    def __stacked(self, generator, discriminator):\n",
    "        discriminator.trainable = False\n",
    "        model = Sequential()\n",
    "        model.add(generator)\n",
    "        model.add(discriminator)\n",
    "        return model\n",
    "    \n",
    "    def compile_models(self):\n",
    "        self.generator = self.__generator()\n",
    "        self.discriminator = self.__discriminator()\n",
    "        self.stacked = self.__stacked(self.generator, self.discriminator)\n",
    "        \n",
    "        self.generator.compile(loss='binary_crossentropy', optimizer=self.OPTIMIZER)\n",
    "        self.discriminator.compile(loss='binary_crossentropy', optimizer=self.OPTIMIZER, metrics=['accuracy'] )\n",
    "        self.stacked.compile(loss='binary_crossentropy', optimizer=self.OPTIMIZER)\n",
    "        \n",
    "        \n",
    "    def save_models(self):\n",
    "        self.generator.save_weights(\"generator_weights.h5\")\n",
    "        self.discriminator.save_weights(\"discriminator_weights.h5\")\n",
    "        self.stacked.save_weights(\"stacked_weights.h5\")\n",
    "        \n",
    "        with open(\"generator_model.json\", \"w\") as json_file:\n",
    "            json_file.write(self.generator.to_json())\n",
    "        \n",
    "        with open(\"discriminator_model.json\", \"w\") as json_file:\n",
    "            json_file.write(self.discriminator.to_json())\n",
    "            \n",
    "        with open(\"stacked_model.json\", \"w\") as json_file:\n",
    "            json_file.write(self.stacked.to_json())\n",
    "            \n",
    "            \n",
    "    def load_models(self):\n",
    "        with open(\"generator_model.json\", \"r\") as json_file:\n",
    "            self.generator = model_from_json(json_file.read())\n",
    "            self.generator.load_weights(\"generator_weights.h5\")\n",
    "            \n",
    "        with open(\"discriminator_model.json\", \"r\") as json_file:\n",
    "            self.discriminator = model_from_json(json_file.read())\n",
    "            self.discriminator.load_weights(\"discriminator_weights.h5\")\n",
    "            \n",
    "        with open(\"stacked_model.json\", \"r\") as json_file:\n",
    "            self.stacked = model_from_json(json_file.read())\n",
    "            self.stacked.load_weights(\"stacked_weights.h5\")\n",
    "            \n",
    "        self.generator.compile(loss='binary_crossentropy', optimizer=self.OPTIMIZER)\n",
    "        self.discriminator.compile(loss='binary_crossentropy', optimizer=self.OPTIMIZER, metrics=['accuracy'] )\n",
    "        self.stacked.compile(loss='binary_crossentropy', optimizer=self.OPTIMIZER)\n",
    "                    \n",
    "    \n",
    "    def train(self,X, y, epochs=200, batch = 100, debug=False):\n",
    "        for cnt in range(epochs):\n",
    "\n",
    "            ## train discriminator\n",
    "            random_index =  np.random.randint(0, len(y) - batch)\n",
    "            X_batch = X[random_index : random_index + batch]\n",
    "            y_batch = y[random_index : random_index + batch]\n",
    "\n",
    "            gen_noise = np.random.normal(0, 1, (batch,1000))\n",
    "            syntetic = self.generator.predict(gen_noise)\n",
    "                \n",
    "            x_combined_batch = np.concatenate((X_batch, syntetic))\n",
    "            y_combined_batch = np.concatenate((y_batch, np.zeros((batch, 1))))\n",
    "\n",
    "            d_loss = self.discriminator.train_on_batch(x_combined_batch, y_combined_batch)\n",
    "\n",
    "            # train generator\n",
    "            noise = np.random.normal(0, 1, (batch,1000))\n",
    "            y_mislabled = np.ones((batch, 1))\n",
    "            g_loss = self.stacked.train_on_batch(noise, y_mislabled)\n",
    "            if debug:\n",
    "                print ('epoch: %d, [Discriminator :: d_loss: %f], [ Generator :: loss: %f]' % (cnt, d_loss[0], g_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, [Discriminator :: d_loss: 0.439209], [ Generator :: loss: 0.973990]\n",
      "epoch: 1, [Discriminator :: d_loss: 0.474114], [ Generator :: loss: 1.001364]\n",
      "epoch: 2, [Discriminator :: d_loss: 0.518967], [ Generator :: loss: 0.939117]\n",
      "epoch: 3, [Discriminator :: d_loss: 0.680071], [ Generator :: loss: 0.849308]\n",
      "epoch: 4, [Discriminator :: d_loss: 0.434392], [ Generator :: loss: 0.821619]\n",
      "epoch: 5, [Discriminator :: d_loss: 0.532563], [ Generator :: loss: 0.939588]\n",
      "epoch: 6, [Discriminator :: d_loss: 0.623354], [ Generator :: loss: 0.935735]\n",
      "epoch: 7, [Discriminator :: d_loss: 0.383403], [ Generator :: loss: 0.862649]\n",
      "epoch: 8, [Discriminator :: d_loss: 0.689868], [ Generator :: loss: 0.841324]\n",
      "epoch: 9, [Discriminator :: d_loss: 0.472486], [ Generator :: loss: 0.804685]\n",
      "epoch: 10, [Discriminator :: d_loss: 0.573614], [ Generator :: loss: 0.849596]\n",
      "epoch: 11, [Discriminator :: d_loss: 0.841441], [ Generator :: loss: 0.836499]\n",
      "epoch: 12, [Discriminator :: d_loss: 0.503512], [ Generator :: loss: 0.747298]\n",
      "epoch: 13, [Discriminator :: d_loss: 0.769654], [ Generator :: loss: 0.803534]\n",
      "epoch: 14, [Discriminator :: d_loss: 0.387386], [ Generator :: loss: 0.888089]\n",
      "epoch: 15, [Discriminator :: d_loss: 0.746747], [ Generator :: loss: 0.784479]\n",
      "epoch: 16, [Discriminator :: d_loss: 0.882066], [ Generator :: loss: 0.816117]\n",
      "epoch: 17, [Discriminator :: d_loss: 0.506795], [ Generator :: loss: 0.855484]\n",
      "epoch: 18, [Discriminator :: d_loss: 0.548520], [ Generator :: loss: 0.769409]\n",
      "epoch: 19, [Discriminator :: d_loss: 0.873914], [ Generator :: loss: 0.766486]\n",
      "epoch: 20, [Discriminator :: d_loss: 0.695554], [ Generator :: loss: 0.759047]\n",
      "epoch: 21, [Discriminator :: d_loss: 0.460453], [ Generator :: loss: 0.771233]\n",
      "epoch: 22, [Discriminator :: d_loss: 0.648120], [ Generator :: loss: 0.744000]\n",
      "epoch: 23, [Discriminator :: d_loss: 0.568697], [ Generator :: loss: 0.738154]\n",
      "epoch: 24, [Discriminator :: d_loss: 0.440842], [ Generator :: loss: 0.753147]\n",
      "epoch: 25, [Discriminator :: d_loss: 0.575117], [ Generator :: loss: 0.796867]\n",
      "epoch: 26, [Discriminator :: d_loss: 0.708827], [ Generator :: loss: 0.806606]\n",
      "epoch: 27, [Discriminator :: d_loss: 0.886693], [ Generator :: loss: 0.757223]\n",
      "epoch: 28, [Discriminator :: d_loss: 0.772524], [ Generator :: loss: 0.769795]\n",
      "epoch: 29, [Discriminator :: d_loss: 0.428816], [ Generator :: loss: 0.747608]\n",
      "epoch: 30, [Discriminator :: d_loss: 0.799320], [ Generator :: loss: 0.784299]\n",
      "epoch: 31, [Discriminator :: d_loss: 0.616447], [ Generator :: loss: 0.784575]\n",
      "epoch: 32, [Discriminator :: d_loss: 0.402836], [ Generator :: loss: 0.766453]\n",
      "epoch: 33, [Discriminator :: d_loss: 0.504147], [ Generator :: loss: 0.743577]\n",
      "epoch: 34, [Discriminator :: d_loss: 0.729857], [ Generator :: loss: 0.740918]\n",
      "epoch: 35, [Discriminator :: d_loss: 0.522821], [ Generator :: loss: 0.763501]\n",
      "epoch: 36, [Discriminator :: d_loss: 0.695245], [ Generator :: loss: 0.777044]\n",
      "epoch: 37, [Discriminator :: d_loss: 0.619387], [ Generator :: loss: 0.765331]\n",
      "epoch: 38, [Discriminator :: d_loss: 0.610222], [ Generator :: loss: 0.745412]\n",
      "epoch: 39, [Discriminator :: d_loss: 0.744876], [ Generator :: loss: 0.746912]\n",
      "epoch: 40, [Discriminator :: d_loss: 0.554412], [ Generator :: loss: 0.748835]\n",
      "epoch: 41, [Discriminator :: d_loss: 0.723618], [ Generator :: loss: 0.720467]\n",
      "epoch: 42, [Discriminator :: d_loss: 0.527873], [ Generator :: loss: 0.723577]\n",
      "epoch: 43, [Discriminator :: d_loss: 0.536666], [ Generator :: loss: 0.758835]\n",
      "epoch: 44, [Discriminator :: d_loss: 0.397174], [ Generator :: loss: 0.712792]\n",
      "epoch: 45, [Discriminator :: d_loss: 0.463930], [ Generator :: loss: 0.751365]\n",
      "epoch: 46, [Discriminator :: d_loss: 0.552694], [ Generator :: loss: 0.737881]\n",
      "epoch: 47, [Discriminator :: d_loss: 0.791680], [ Generator :: loss: 0.722939]\n",
      "epoch: 48, [Discriminator :: d_loss: 0.747073], [ Generator :: loss: 0.717358]\n",
      "epoch: 49, [Discriminator :: d_loss: 0.458635], [ Generator :: loss: 0.740746]\n",
      "epoch: 50, [Discriminator :: d_loss: 0.477874], [ Generator :: loss: 0.727957]\n",
      "epoch: 51, [Discriminator :: d_loss: 0.531334], [ Generator :: loss: 0.742444]\n",
      "epoch: 52, [Discriminator :: d_loss: 0.514530], [ Generator :: loss: 0.723650]\n",
      "epoch: 53, [Discriminator :: d_loss: 0.728625], [ Generator :: loss: 0.792622]\n",
      "epoch: 54, [Discriminator :: d_loss: 0.548474], [ Generator :: loss: 0.729243]\n",
      "epoch: 55, [Discriminator :: d_loss: 0.575168], [ Generator :: loss: 0.724714]\n",
      "epoch: 56, [Discriminator :: d_loss: 0.711424], [ Generator :: loss: 0.746690]\n",
      "epoch: 57, [Discriminator :: d_loss: 0.697954], [ Generator :: loss: 0.749345]\n",
      "epoch: 58, [Discriminator :: d_loss: 0.551949], [ Generator :: loss: 0.757870]\n",
      "epoch: 59, [Discriminator :: d_loss: 0.503415], [ Generator :: loss: 0.726412]\n",
      "epoch: 60, [Discriminator :: d_loss: 0.756069], [ Generator :: loss: 0.728565]\n",
      "epoch: 61, [Discriminator :: d_loss: 0.616254], [ Generator :: loss: 0.739929]\n",
      "epoch: 62, [Discriminator :: d_loss: 0.576746], [ Generator :: loss: 0.747143]\n",
      "epoch: 63, [Discriminator :: d_loss: 0.849035], [ Generator :: loss: 0.727414]\n",
      "epoch: 64, [Discriminator :: d_loss: 0.876801], [ Generator :: loss: 0.750674]\n",
      "epoch: 65, [Discriminator :: d_loss: 0.387279], [ Generator :: loss: 0.758525]\n",
      "epoch: 66, [Discriminator :: d_loss: 0.876794], [ Generator :: loss: 0.745410]\n",
      "epoch: 67, [Discriminator :: d_loss: 0.850502], [ Generator :: loss: 0.782863]\n",
      "epoch: 68, [Discriminator :: d_loss: 0.373797], [ Generator :: loss: 0.774431]\n",
      "epoch: 69, [Discriminator :: d_loss: 0.489868], [ Generator :: loss: 0.725206]\n",
      "epoch: 70, [Discriminator :: d_loss: 0.675415], [ Generator :: loss: 0.756653]\n",
      "epoch: 71, [Discriminator :: d_loss: 0.886592], [ Generator :: loss: 0.732650]\n",
      "epoch: 72, [Discriminator :: d_loss: 0.568352], [ Generator :: loss: 0.733764]\n",
      "epoch: 73, [Discriminator :: d_loss: 0.833149], [ Generator :: loss: 0.739583]\n",
      "epoch: 74, [Discriminator :: d_loss: 0.854486], [ Generator :: loss: 0.729911]\n",
      "epoch: 75, [Discriminator :: d_loss: 0.538709], [ Generator :: loss: 0.725455]\n",
      "epoch: 76, [Discriminator :: d_loss: 0.482069], [ Generator :: loss: 0.768964]\n",
      "epoch: 77, [Discriminator :: d_loss: 0.707734], [ Generator :: loss: 0.785559]\n",
      "epoch: 78, [Discriminator :: d_loss: 0.487750], [ Generator :: loss: 0.770660]\n",
      "epoch: 79, [Discriminator :: d_loss: 0.554734], [ Generator :: loss: 0.764233]\n",
      "epoch: 80, [Discriminator :: d_loss: 0.700859], [ Generator :: loss: 0.726506]\n",
      "epoch: 81, [Discriminator :: d_loss: 0.840645], [ Generator :: loss: 0.732584]\n",
      "epoch: 82, [Discriminator :: d_loss: 0.538710], [ Generator :: loss: 0.719960]\n",
      "epoch: 83, [Discriminator :: d_loss: 0.472857], [ Generator :: loss: 0.767820]\n",
      "epoch: 84, [Discriminator :: d_loss: 0.558924], [ Generator :: loss: 0.771925]\n",
      "epoch: 85, [Discriminator :: d_loss: 0.771393], [ Generator :: loss: 0.756131]\n",
      "epoch: 86, [Discriminator :: d_loss: 0.402809], [ Generator :: loss: 0.795957]\n",
      "epoch: 87, [Discriminator :: d_loss: 0.371034], [ Generator :: loss: 0.782070]\n",
      "epoch: 88, [Discriminator :: d_loss: 0.568936], [ Generator :: loss: 0.759557]\n",
      "epoch: 89, [Discriminator :: d_loss: 0.443827], [ Generator :: loss: 0.729301]\n",
      "epoch: 90, [Discriminator :: d_loss: 0.432686], [ Generator :: loss: 0.785825]\n",
      "epoch: 91, [Discriminator :: d_loss: 0.644441], [ Generator :: loss: 0.725984]\n",
      "epoch: 92, [Discriminator :: d_loss: 0.474002], [ Generator :: loss: 0.724676]\n",
      "epoch: 93, [Discriminator :: d_loss: 0.517401], [ Generator :: loss: 0.733287]\n",
      "epoch: 94, [Discriminator :: d_loss: 0.540462], [ Generator :: loss: 0.776982]\n",
      "epoch: 95, [Discriminator :: d_loss: 0.714927], [ Generator :: loss: 0.727608]\n",
      "epoch: 96, [Discriminator :: d_loss: 0.518211], [ Generator :: loss: 0.814483]\n",
      "epoch: 97, [Discriminator :: d_loss: 0.508777], [ Generator :: loss: 0.740461]\n",
      "epoch: 98, [Discriminator :: d_loss: 0.536790], [ Generator :: loss: 0.763332]\n",
      "epoch: 99, [Discriminator :: d_loss: 0.774170], [ Generator :: loss: 0.732850]\n"
     ]
    }
   ],
   "source": [
    "gan = GAN(X.shape[1])\n",
    "gan.train(X, y, debug=True, epochs=100, batch=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preidiction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = gan.discriminator.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.745"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(y == (predict > 0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0 100]\n",
      " [  2 298]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics \n",
    "print(metrics.confusion_matrix(y, (predict > 0.5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.755"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.accuracy_score(y, (predict > 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=17) \n",
    "cvscores = [] \n",
    "for train, test in kfold.split(X, y): \n",
    "    gan = GAN(X.shape[1]) \n",
    "    gan.train(X[train], y[train], epochs=100, batch=20, debug=False) \n",
    "    predict = gan.discriminator.predict(X[test]) \n",
    "    acc = metrics.f1_score(y[test], (predict > 0.5)) \n",
    "    cvscores.append(acc) \n",
    "    print(acc) \n",
    "print(\"%.2f%% (+/- %.2f%%)\" % (np.mean(cvscores), np.std(cvscores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.853868194842407"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.f1_score(y, (predict > 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "gan.save_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.745"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gan.load_models()\n",
    "predict = gan.discriminator.predict(X)\n",
    "np.mean(y == (predict > 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
