{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nsuprotivniy/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys, os, pickle\n",
    "from keras.models import Sequential\n",
    "import pandas as pd\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.optimizers import Adam, RMSprop, SGD\n",
    "import sys, re, glob\n",
    "import numpy as np\n",
    "from keras.layers import Input, Dense, Flatten\n",
    "import sys, re, glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from lxml import etree\n",
    "from sklearn.model_selection import StratifiedKFold "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "proto_dict = {\n",
    "    'ns3::WifiMacHeader' : 'MAC',\n",
    "    'ns3::LlcSnapHeader' : 'LLC',\n",
    "    'ns3::ArpHeader' : 'ARP',\n",
    "    'ns3::Ipv4Header' : 'IPv4',\n",
    "    'ns3::UdpHeader' : 'UDP',\n",
    "    'ns3::aodv::TypeHeader' : 'AODV_Type',\n",
    "    'ns3::aodv::RrepHeader' : 'AODV_RREP',\n",
    "    'ns3::aodv::RreqHeader' : 'AODV_RREQ',\n",
    "    'ns3::aodv::RrepAckHeader' : 'AODV_RACK',\n",
    "    'ns3::aodv::RerrHeader' : 'AODV_RERR',\n",
    "    'ns3::Icmpv4Header' : 'ICMPv4_HEADER',\n",
    "    'ns3::Icmpv4TimeExceeded' : 'ICMPv4_TE',\n",
    "    'ns3::Icmpv4DestinationUnreachable' : 'ICMPv4_DU'\n",
    "}\n",
    "\n",
    "def parce_packets(input_file):\n",
    "    with open(input_file, 'r') as f:\n",
    "        data = f.read()\n",
    "    data = data.split('meta-info=\"')\n",
    "    meta_info = []\n",
    "    data.pop(0)\n",
    "    for b in data:\n",
    "        meta_info.append(b.split('\"')[0])\n",
    "    packets = []\n",
    "    for m in meta_info:\n",
    "        raw = {}\n",
    "        for proto in proto_dict:\n",
    "            if m.find(proto) != -1:\n",
    "                raw[proto_dict[proto]] = m.split(proto+' (')[1].split(') ns3')[0]\n",
    "        p = {}\n",
    "        if 'MAC' in raw and 'LLC' in raw:\n",
    "            p['MAC'] = {}\n",
    "            p['MAC']['src'] = raw['MAC'].split('SA=')[1].split(',')[0]\n",
    "            p['MAC']['dst'] = raw['MAC'].split('DA=')[1].split(',')[0]\n",
    "            p['MAC']['type'] = int(raw['LLC'].split('type ')[1], 16)\n",
    "        if 'ARP' in raw:\n",
    "            p['ARP'] = {}\n",
    "            if raw['ARP'].find('request') != -1:\n",
    "                p['ARP']['type'] = 'request'\n",
    "            else:\n",
    "                p['ARP']['type'] = 'reply'\n",
    "                p['ARP']['MAC_dst'] = raw['ARP'].split('dest mac: ')[1].split(' ')[0]\n",
    "            p['ARP']['MAC_src'] = raw['ARP'].split('source mac: ')[1].split(' ')[0]\n",
    "            p['ARP']['IP_src'] = raw['ARP'].split('source ipv4: ')[1].split(' ')[0]\n",
    "            p['ARP']['IP_dst'] = raw['ARP'].split('dest ipv4: ')[1]\n",
    "        if 'IPv4' in raw:\n",
    "            p['IPv4'] = {}\n",
    "            ipv4data = raw['IPv4'].split(' ')\n",
    "            p['IPv4']['src'] = ipv4data[-3]\n",
    "            p['IPv4']['dst'] = ipv4data[-1]\n",
    "            p['IPv4']['proto'] = int(raw['IPv4'].split('protocol ')[1].split(' ')[0])\n",
    "            p['IPv4']['ttl'] = int(raw['IPv4'].split('ttl ')[1].split(' ')[0])\n",
    "            p['IPv4']['length'] = int(raw['IPv4'].split('length: ')[1].split(' ')[0])\n",
    "        if 'UDP' in raw:\n",
    "            p['UDP'] = {}\n",
    "            p['UDP']['length'] = int(raw['UDP'].split('length: ')[1].split(' ')[0])\n",
    "        if 'AODV_Type' in raw:\n",
    "            p['AODV'] = {}\n",
    "            p['AODV']['type'] = raw['AODV_Type']\n",
    "        if 'AODV_RREP' in raw:\n",
    "            p['AODV']['RREP'] = {}\n",
    "            p['AODV']['RREP']['dst'] = raw['AODV_RREP'].split('destination: ipv4 ')[1].split(' ')[0]\n",
    "            p['AODV']['RREP']['src'] = raw['AODV_RREP'].split('source ipv4 ')[1].split(' ')[0]\n",
    "            p['AODV']['RREP']['SN'] = int(raw['AODV_RREP'].split('sequence number ')[1].split(' ')[0])\n",
    "            p['AODV']['RREP']['lifetime'] = int(raw['AODV_RREP'].split('lifetime ')[1].split(' ')[0])\n",
    "            p['AODV']['RREP']['acknowledgment'] = raw['AODV_RREP'].split('acknowledgment ')[1].split(' ')[0]\n",
    "            p['AODV']['RREP']['flag'] = raw['AODV_RREP'].split('flag ')[1]\n",
    "        if 'AODV_RREQ' in raw:\n",
    "            p['AODV']['RREQ'] = {}\n",
    "            p['AODV']['RREQ']['ID'] = int(raw['AODV_RREQ'].split('ID ')[1].split(' ')[0])\n",
    "            p['AODV']['RREQ']['dst'] = raw['AODV_RREQ'].split('destination: ipv4 ')[1].split(' ')[0]\n",
    "            p['AODV']['RREQ']['src'] = raw['AODV_RREQ'].split('source: ipv4 ')[1].split(' ')[0]\n",
    "            p['AODV']['RREQ']['SN'] = int(raw['AODV_RREQ'].split('sequence number ')[1].split(' ')[0])\n",
    "            p['AODV']['RREQ']['flags'] = raw['AODV_RREQ'].split('flags: ')[1]\n",
    "        if 'AODV_RACK' in raw:\n",
    "            pass # seems to be empty\n",
    "        if 'AODV_RERR' in raw:\n",
    "            p['AODV']['RERR'] = raw['AODV_RERR']\n",
    "        if 'ICMPv4_HEADER' in raw:\n",
    "            p['ICMPv4'] = {}\n",
    "            p['ICMPv4']['type'] = int(raw['ICMPv4_HEADER'].split('type=')[1].split(',')[0])\n",
    "            p['ICMPv4']['code'] = int(raw['ICMPv4_HEADER'].split('code=')[1])\n",
    "        if 'ICMPv4_TE' in raw:\n",
    "            p['ICMPv4']['data'] = raw['ICMPv4_TE']\n",
    "        if 'ICMPv4_DU' in raw:\n",
    "            p['ICMPv4']['data'] = raw['ICMPv4_DU']\n",
    "\n",
    "        if len(p) > 0:\n",
    "            packets.append(p)\n",
    "    return packets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mac_to_id(mac):\n",
    "    return int(mac.split(':')[-1], 16) - 1\n",
    "\n",
    "def ip_to_id(ip):\n",
    "    return int(ip.split('.')[-1]) - 1\n",
    "\n",
    "def entropy(arr):\n",
    "    s = arr.sum()\n",
    "    arr = arr / s\n",
    "    arr = arr * np.log(arr) / np.log(2)\n",
    "    return -(arr.sum() / np.log(len(arr)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path, rng):\n",
    "    routes = list()\n",
    "    flows = list()\n",
    "    packets = list()\n",
    "\n",
    "    for i in range(rng):\n",
    "\n",
    "        for routes_file in glob.glob('{}/output_{}/*.routes'.format(path, i)):\n",
    "\n",
    "            info = re.findall('(\\w+)/output_(\\d+)', routes_file)\n",
    "\n",
    "            handle = open(routes_file, 'r')\n",
    "            data = handle.read()\n",
    "            handle.close()\n",
    "\n",
    "            nodes = re.split('\\n\\n', data)\n",
    "            nodes.pop()\n",
    "\n",
    "            for node in nodes:\n",
    "\n",
    "                header = re.findall('Node:\\s+(\\d+)\\s+Time:\\s+(\\d+)', node)\n",
    "                lines = re.findall('(\\d{1,3}(?:\\.\\d{1,3}){3})\\s+(\\d{1,3}(?:\\.\\d{1,3}){3})\\s+(\\d{1,3}(?:\\.\\d{1,3}){3})\\s+(\\w+)\\s+(-?\\d+\\.\\d+)\\s+(\\d+)', node)\n",
    "\n",
    "                for line in lines:\n",
    "                    l = list(line)\n",
    "                    l[4] = float(line[4])\n",
    "                    l[5] = int(line[5])\n",
    "                    routes.append(header[0] + tuple(l) + info[0])\n",
    "\n",
    "        for flowmon_file in glob.glob('{}/output_{}/*.flowmon'.format(path, i)):\n",
    "\n",
    "            info = re.findall('(\\w+)/output_(\\d+)', flowmon_file)\n",
    "\n",
    "            with open(flowmon_file) as fobj:\n",
    "                xml = fobj.read()\n",
    "\n",
    "            root = etree.fromstring(xml)\n",
    "\n",
    "\n",
    "            for flow in root.xpath('/FlowMonitor/FlowStats/Flow'):\n",
    "\n",
    "                attributes = list()\n",
    "\n",
    "                for attrib in flow.attrib:\n",
    "\n",
    "                    attr = flow.attrib[attrib]\n",
    "                    if 'ns' in attr:\n",
    "                        attr = re.findall('(\\d+)', attr)[0]\n",
    "\n",
    "                    attributes.append(int(attr))\n",
    "\n",
    "                flows.append(tuple(attributes) + info[0])\n",
    "\n",
    "        for packets_file in glob.glob('{}/output_{}/*.xml'.format(path, i)):\n",
    "\n",
    "            if \"routingtable-wireless.xml\" in packets_file:\n",
    "                continue\n",
    "\n",
    "            info = re.findall('(\\w+)/output_(\\d+)', packets_file)\n",
    "\n",
    "            packets_dict = parce_packets(packets_file)\n",
    "            \n",
    "            table_packets = []\n",
    "            for p in packets_dict:\n",
    "                if 'IPv4' in p:\n",
    "                    table_packets.append([mac_to_id(p['MAC']['src']), mac_to_id(p['MAC']['dst']), ip_to_id(p['IPv4']['src']), ip_to_id(p['IPv4']['dst'])])\n",
    "\n",
    "            table_packets = pd.DataFrame(table_packets, columns=['mac_src', 'mac_dst', 'ip_src', 'ip_dst'])\n",
    "            table_packets = table_packets[table_packets['ip_dst'] != 254]\n",
    "            table_packets['input'] = 0\n",
    "            table_packets['output'] = 0\n",
    "            input_cnt = table_packets.groupby('mac_src').agg({'input':'count'})\n",
    "            output_cnt = table_packets.groupby('mac_dst').agg({'output':'count'})\n",
    "            io_mac = input_cnt.join(output_cnt)\n",
    "            io_mac[\"diff\"] = io_mac['input'] - io_mac['output']\n",
    "            io_mac[\"normalized\"] = io_mac[\"diff\"] - io_mac[\"diff\"].min() + 1\n",
    "            \n",
    "            input = io_mac.agg({'input': [entropy, 'min', 'max', 'mean', 'var']}).values.flatten()\n",
    "            output = io_mac.agg({'output': [entropy, 'min', 'max', 'mean', 'var']}).values.flatten()\n",
    "            diff = io_mac.agg({'diff': ['min', 'max', 'mean', 'var']}).values.flatten()\n",
    "            normalized = io_mac.agg({'normalized': [entropy, 'min', 'max', 'mean', 'var']}).values.flatten()\n",
    "            src_count = table_packets[table_packets['mac_src'] == table_packets['ip_src']].count()[0]\n",
    "            dst_count = table_packets[table_packets['mac_dst'] == table_packets['ip_dst']].count()[0]\n",
    "\n",
    "            packets.append(np.concatenate((input, output, diff, normalized, [src_count, dst_count], info[0])))\n",
    "\n",
    "\n",
    "    routes_table = pd.DataFrame(routes, columns=['Node', 'Time', 'Destination', 'Gateway', 'Interface', 'Flag', 'Expire', 'Hops', 'Type', 'Test'])\n",
    "\n",
    "    #print(routes_table)\n",
    "    \n",
    "    flag_count = routes_table.groupby(['Type', 'Test', 'Time', 'Flag']).agg({'Flag' : ['count']})\n",
    "    flag_count = flag_count.reset_index(col_level=1)\n",
    "    flag_count.columns = flag_count.columns.droplevel()\n",
    "    flag_agg = flag_count.groupby(['Type', 'Test', 'Flag']).agg({'count': ['max', 'min', 'mean', 'var']}).unstack().reset_index()\n",
    "    flag_agg.columns = [col[0]+col[1]+col[2] for col in flag_agg.columns]\n",
    "    flag_agg = flag_agg.set_index(['Type', 'Test'])\n",
    "\n",
    "    hops_node_dest_agg = routes_table.groupby(['Type', 'Test', 'Node', 'Destination']).agg({'Hops' : ['min', 'max', 'mean']})\n",
    "    hops_node_agg = hops_node_dest_agg.reset_index().groupby([\"Type\", \"Test\", \"Node\"]).agg(['min', 'max', 'mean']).reset_index(col_level=1)\n",
    "    hops_node_agg.columns = hops_node_agg.columns.droplevel()\n",
    "    hops_agg = hops_node_agg.reset_index().groupby([\"Type\", \"Test\"]).agg(['min', 'max', 'mean']).reset_index(col_level=1)\n",
    "    hops_agg.columns = [col[0]+col[1]+col[2] for col in hops_agg.columns]\n",
    "    hops_agg = hops_agg.set_index(['Type', 'Test'])\n",
    "\n",
    "    flows_table = pd.DataFrame(flows, columns=['flowId', 'timeFirstTxPacket', 'timeFirstRxPacket', 'timeLastTxPacket', 'timeLastRxPacket', 'delaySum', 'jitterSum', 'lastDelay', 'txBytes', 'rxBytes', 'txPackets', 'rxPackets', 'lostPackets', 'timesForwarded', 'Type', 'Test'])\n",
    "\n",
    "    lost_agg = flows_table.groupby(['Type', 'Test']).agg({'lostPackets' : ['sum', 'mean']})\n",
    "    forwarded_agg = flows_table.groupby(['Type', 'Test']).agg({'timesForwarded' : ['sum', 'max', 'mean', 'var']})\n",
    "\n",
    "    packets_table = pd.DataFrame(packets, columns=[\"entopy_input\", \"min_input\", \"max_input\", \"mean_input\", \"var_input\", \"entopy_output\", \"min_output\", \"max_output\", \"mean_output\", \"var_output\", \"min_diff\", \"max_diff\", \"mean_diff\", \"var_diff\", \"entopy_normolized\", \"min_normolized\", \"max_normolized\", \"mean_normolized\", \"var_normolized\", \"src_count\", \"dst_count\", 'Type', 'Test'])\n",
    "    packets_table = packets_table.set_index(['Type', 'Test'])\n",
    "\n",
    "    return flag_agg.join(hops_agg).join(lost_agg).join(forwarded_agg).join(packets_table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nsuprotivniy/anaconda3/lib/python3.6/site-packages/pandas/core/generic.py:2530: PerformanceWarning: dropping on a non-lexsorted multi-index without a level parameter may impact performance.\n",
      "  obj = obj._drop_axis(labels, axis, level=level, errors=errors)\n",
      "/home/nsuprotivniy/anaconda3/lib/python3.6/site-packages/pandas/core/reshape/merge.py:558: UserWarning: merging between different levels can give an unintended result (1 levels on the left, 2 on the right)\n",
      "  warnings.warn(msg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "normal = load_data(\"../data/normal\", 100)\n",
    "blackhole = load_data(\"../data/blackhole\", 100)\n",
    "grayhole = load_data(\"../data/greyhole\", 100)\n",
    "wormhole = load_data(\"../data/wormhole\", 100)\n",
    "ddos = load_data(\"../data/ddos\", 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.concatenate((normal, blackhole, grayhole, ddos)).astype(np.float)\n",
    "y = np.concatenate((np.zeros((100, 1)), np.zeros((300, 1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes = np.array(range(len(y)))\n",
    "np.random.shuffle(indexes)\n",
    "indexes\n",
    "X = X[indexes]\n",
    "y = y[indexes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[np.isnan(X)] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAN:\n",
    "    def __init__(self, shape):\n",
    "        self.SHAPE = shape\n",
    "        self.OPTIMIZER = Adam()\n",
    "        self.compile_models()\n",
    "        \n",
    "    def __generator(self):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(2048, input_shape=(1000,)))\n",
    "        model.add(Dense(4096))\n",
    "        model.add(Dense(4096))\n",
    "        model.add(Dense(4096))\n",
    "        model.add(Dense(4096))\n",
    "        model.add(Dense(4096))\n",
    "        model.add(Dense(4096))\n",
    "        model.add(Dense(self.SHAPE, activation='sigmoid'))\n",
    "        return model\n",
    "    \n",
    "    def __discriminator(self):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(10, input_shape=(self.SHAPE,)))\n",
    "        model.add(Dense(4096))\n",
    "        model.add(Dense(4096))\n",
    "        model.add(Dense(4096))\n",
    "        model.add(Dense(4096))\n",
    "        model.add(Dense(4096))\n",
    "        model.add(Dense(4096))\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "        return model\n",
    "    \n",
    "    def __stacked(self, generator, discriminator):\n",
    "        discriminator.trainable = False\n",
    "        model = Sequential()\n",
    "        model.add(generator)\n",
    "        model.add(discriminator)\n",
    "        return model\n",
    "    \n",
    "    def compile_models(self):\n",
    "        self.generator = self.__generator()\n",
    "        self.discriminator = self.__discriminator()\n",
    "        self.stacked = self.__stacked(self.generator, self.discriminator)\n",
    "        \n",
    "        self.generator.compile(loss='binary_crossentropy', optimizer=self.OPTIMIZER)\n",
    "        self.discriminator.compile(loss='binary_crossentropy', optimizer=self.OPTIMIZER, metrics=['accuracy'] )\n",
    "        self.stacked.compile(loss='binary_crossentropy', optimizer=self.OPTIMIZER)\n",
    "        \n",
    "    \n",
    "    def train(self,X, y, epochs=200, batch = 100, debug=False):\n",
    "        for cnt in range(epochs):\n",
    "\n",
    "            ## train discriminator\n",
    "            random_index =  np.random.randint(0, len(y) - batch)\n",
    "            X_batch = X[random_index : random_index + batch]\n",
    "            y_batch = y[random_index : random_index + batch]\n",
    "\n",
    "            gen_noise = np.random.normal(0, 1, (batch,1000))\n",
    "            syntetic = self.generator.predict(gen_noise)\n",
    "                \n",
    "            x_combined_batch = np.concatenate((X_batch, syntetic))\n",
    "            y_combined_batch = np.concatenate((y_batch, np.zeros((batch, 1))))\n",
    "\n",
    "            d_loss = self.discriminator.train_on_batch(x_combined_batch, y_combined_batch)\n",
    "\n",
    "            # train generator\n",
    "            noise = np.random.normal(0, 1, (batch,1000))\n",
    "            y_mislabled = np.ones((batch, 1))\n",
    "            g_loss = self.stacked.train_on_batch(noise, y_mislabled)\n",
    "            if debug:\n",
    "                print ('epoch: %d, [Discriminator :: d_loss: %f], [ Generator :: loss: %f]' % (cnt, d_loss[0], g_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, [Discriminator :: d_loss: 0.964235], [ Generator :: loss: 0.692653]\n",
      "epoch: 1, [Discriminator :: d_loss: 1.027603], [ Generator :: loss: 0.695584]\n",
      "epoch: 2, [Discriminator :: d_loss: 3.078485], [ Generator :: loss: 0.655927]\n",
      "epoch: 3, [Discriminator :: d_loss: 1.815973], [ Generator :: loss: 0.714323]\n",
      "epoch: 4, [Discriminator :: d_loss: 2.990814], [ Generator :: loss: 0.675827]\n",
      "epoch: 5, [Discriminator :: d_loss: 1.439198], [ Generator :: loss: 0.735244]\n",
      "epoch: 6, [Discriminator :: d_loss: 1.411734], [ Generator :: loss: 0.696514]\n",
      "epoch: 7, [Discriminator :: d_loss: 0.803104], [ Generator :: loss: 0.753525]\n",
      "epoch: 8, [Discriminator :: d_loss: 1.961636], [ Generator :: loss: 0.655227]\n",
      "epoch: 9, [Discriminator :: d_loss: 2.018921], [ Generator :: loss: 0.642599]\n",
      "epoch: 10, [Discriminator :: d_loss: 2.063901], [ Generator :: loss: 0.728903]\n",
      "epoch: 11, [Discriminator :: d_loss: 1.883289], [ Generator :: loss: 0.693898]\n",
      "epoch: 12, [Discriminator :: d_loss: 1.958215], [ Generator :: loss: 0.727894]\n",
      "epoch: 13, [Discriminator :: d_loss: 0.947159], [ Generator :: loss: 0.692323]\n",
      "epoch: 14, [Discriminator :: d_loss: 1.329300], [ Generator :: loss: 0.725596]\n",
      "epoch: 15, [Discriminator :: d_loss: 1.261939], [ Generator :: loss: 0.680482]\n",
      "epoch: 16, [Discriminator :: d_loss: 1.394264], [ Generator :: loss: 0.649270]\n",
      "epoch: 17, [Discriminator :: d_loss: 1.678957], [ Generator :: loss: 0.729539]\n",
      "epoch: 18, [Discriminator :: d_loss: 1.304254], [ Generator :: loss: 0.661641]\n",
      "epoch: 19, [Discriminator :: d_loss: 2.964962], [ Generator :: loss: 0.748515]\n",
      "epoch: 20, [Discriminator :: d_loss: 1.992721], [ Generator :: loss: 0.708361]\n",
      "epoch: 21, [Discriminator :: d_loss: 1.382767], [ Generator :: loss: 0.750458]\n",
      "epoch: 22, [Discriminator :: d_loss: 2.550962], [ Generator :: loss: 0.694656]\n",
      "epoch: 23, [Discriminator :: d_loss: 1.543923], [ Generator :: loss: 0.717384]\n",
      "epoch: 24, [Discriminator :: d_loss: 1.867228], [ Generator :: loss: 0.689149]\n",
      "epoch: 25, [Discriminator :: d_loss: 2.600363], [ Generator :: loss: 0.641658]\n",
      "epoch: 26, [Discriminator :: d_loss: 2.314597], [ Generator :: loss: 0.723088]\n",
      "epoch: 27, [Discriminator :: d_loss: 1.667896], [ Generator :: loss: 0.655823]\n",
      "epoch: 28, [Discriminator :: d_loss: 2.312739], [ Generator :: loss: 0.679925]\n",
      "epoch: 29, [Discriminator :: d_loss: 2.377112], [ Generator :: loss: 0.713554]\n",
      "epoch: 30, [Discriminator :: d_loss: 1.168898], [ Generator :: loss: 0.724330]\n",
      "epoch: 31, [Discriminator :: d_loss: 1.014503], [ Generator :: loss: 0.647605]\n",
      "epoch: 32, [Discriminator :: d_loss: 1.546187], [ Generator :: loss: 0.662985]\n",
      "epoch: 33, [Discriminator :: d_loss: 3.048336], [ Generator :: loss: 0.738917]\n",
      "epoch: 34, [Discriminator :: d_loss: 1.338460], [ Generator :: loss: 0.663801]\n",
      "epoch: 35, [Discriminator :: d_loss: 0.988943], [ Generator :: loss: 0.727665]\n",
      "epoch: 36, [Discriminator :: d_loss: 1.311880], [ Generator :: loss: 0.711154]\n",
      "epoch: 37, [Discriminator :: d_loss: 1.987729], [ Generator :: loss: 0.746499]\n",
      "epoch: 38, [Discriminator :: d_loss: 1.674473], [ Generator :: loss: 0.677893]\n",
      "epoch: 39, [Discriminator :: d_loss: 1.267371], [ Generator :: loss: 0.752960]\n",
      "epoch: 40, [Discriminator :: d_loss: 1.977045], [ Generator :: loss: 0.749057]\n",
      "epoch: 41, [Discriminator :: d_loss: 2.000262], [ Generator :: loss: 0.730274]\n",
      "epoch: 42, [Discriminator :: d_loss: 0.372723], [ Generator :: loss: 0.674543]\n",
      "epoch: 43, [Discriminator :: d_loss: 2.586319], [ Generator :: loss: 0.656705]\n",
      "epoch: 44, [Discriminator :: d_loss: 2.323659], [ Generator :: loss: 0.790487]\n",
      "epoch: 45, [Discriminator :: d_loss: 2.629262], [ Generator :: loss: 0.800726]\n",
      "epoch: 46, [Discriminator :: d_loss: 1.327484], [ Generator :: loss: 0.744424]\n",
      "epoch: 47, [Discriminator :: d_loss: 0.983751], [ Generator :: loss: 0.707604]\n",
      "epoch: 48, [Discriminator :: d_loss: 1.250862], [ Generator :: loss: 0.679444]\n",
      "epoch: 49, [Discriminator :: d_loss: 2.312262], [ Generator :: loss: 0.691854]\n",
      "epoch: 50, [Discriminator :: d_loss: 1.845786], [ Generator :: loss: 0.689877]\n",
      "epoch: 51, [Discriminator :: d_loss: 1.907998], [ Generator :: loss: 0.667154]\n",
      "epoch: 52, [Discriminator :: d_loss: 3.057867], [ Generator :: loss: 0.711886]\n",
      "epoch: 53, [Discriminator :: d_loss: 1.285843], [ Generator :: loss: 0.767430]\n",
      "epoch: 54, [Discriminator :: d_loss: 1.416804], [ Generator :: loss: 0.704912]\n",
      "epoch: 55, [Discriminator :: d_loss: 2.189615], [ Generator :: loss: 0.676715]\n",
      "epoch: 56, [Discriminator :: d_loss: 1.141990], [ Generator :: loss: 0.741675]\n",
      "epoch: 57, [Discriminator :: d_loss: 1.857371], [ Generator :: loss: 0.756945]\n",
      "epoch: 58, [Discriminator :: d_loss: 1.563936], [ Generator :: loss: 0.743253]\n",
      "epoch: 59, [Discriminator :: d_loss: 2.414382], [ Generator :: loss: 0.660506]\n",
      "epoch: 60, [Discriminator :: d_loss: 1.340388], [ Generator :: loss: 0.678438]\n",
      "epoch: 61, [Discriminator :: d_loss: 2.572916], [ Generator :: loss: 0.626015]\n",
      "epoch: 62, [Discriminator :: d_loss: 2.623664], [ Generator :: loss: 0.712508]\n",
      "epoch: 63, [Discriminator :: d_loss: 2.224410], [ Generator :: loss: 0.710932]\n",
      "epoch: 64, [Discriminator :: d_loss: 2.553041], [ Generator :: loss: 0.777132]\n",
      "epoch: 65, [Discriminator :: d_loss: 1.947944], [ Generator :: loss: 0.659287]\n",
      "epoch: 66, [Discriminator :: d_loss: 1.416565], [ Generator :: loss: 0.704420]\n",
      "epoch: 67, [Discriminator :: d_loss: 0.968300], [ Generator :: loss: 0.747020]\n",
      "epoch: 68, [Discriminator :: d_loss: 1.879822], [ Generator :: loss: 0.695934]\n",
      "epoch: 69, [Discriminator :: d_loss: 2.243871], [ Generator :: loss: 0.697620]\n",
      "epoch: 70, [Discriminator :: d_loss: 1.467590], [ Generator :: loss: 0.678527]\n",
      "epoch: 71, [Discriminator :: d_loss: 2.665318], [ Generator :: loss: 0.774511]\n",
      "epoch: 72, [Discriminator :: d_loss: 1.013540], [ Generator :: loss: 0.698333]\n",
      "epoch: 73, [Discriminator :: d_loss: 1.240788], [ Generator :: loss: 0.736422]\n",
      "epoch: 74, [Discriminator :: d_loss: 3.375249], [ Generator :: loss: 0.677960]\n",
      "epoch: 75, [Discriminator :: d_loss: 1.559558], [ Generator :: loss: 0.726531]\n",
      "epoch: 76, [Discriminator :: d_loss: 0.790214], [ Generator :: loss: 0.725134]\n",
      "epoch: 77, [Discriminator :: d_loss: 2.286047], [ Generator :: loss: 0.787839]\n",
      "epoch: 78, [Discriminator :: d_loss: 2.403097], [ Generator :: loss: 0.747575]\n",
      "epoch: 79, [Discriminator :: d_loss: 1.528307], [ Generator :: loss: 0.673270]\n",
      "epoch: 80, [Discriminator :: d_loss: 1.567942], [ Generator :: loss: 0.711003]\n",
      "epoch: 81, [Discriminator :: d_loss: 1.408556], [ Generator :: loss: 0.717106]\n",
      "epoch: 82, [Discriminator :: d_loss: 2.565892], [ Generator :: loss: 0.695127]\n",
      "epoch: 83, [Discriminator :: d_loss: 0.370110], [ Generator :: loss: 0.761007]\n",
      "epoch: 84, [Discriminator :: d_loss: 2.518757], [ Generator :: loss: 0.803300]\n",
      "epoch: 85, [Discriminator :: d_loss: 0.876180], [ Generator :: loss: 0.751575]\n",
      "epoch: 86, [Discriminator :: d_loss: 2.815265], [ Generator :: loss: 0.693278]\n",
      "epoch: 87, [Discriminator :: d_loss: 1.538393], [ Generator :: loss: 0.742594]\n",
      "epoch: 88, [Discriminator :: d_loss: 1.272408], [ Generator :: loss: 0.696837]\n",
      "epoch: 89, [Discriminator :: d_loss: 2.231566], [ Generator :: loss: 0.715169]\n",
      "epoch: 90, [Discriminator :: d_loss: 1.576083], [ Generator :: loss: 0.712848]\n",
      "epoch: 91, [Discriminator :: d_loss: 2.317252], [ Generator :: loss: 0.736570]\n",
      "epoch: 92, [Discriminator :: d_loss: 2.438120], [ Generator :: loss: 0.653579]\n",
      "epoch: 93, [Discriminator :: d_loss: 2.273145], [ Generator :: loss: 0.679706]\n",
      "epoch: 94, [Discriminator :: d_loss: 0.972714], [ Generator :: loss: 0.677783]\n",
      "epoch: 95, [Discriminator :: d_loss: 2.248799], [ Generator :: loss: 0.743162]\n",
      "epoch: 96, [Discriminator :: d_loss: 1.271526], [ Generator :: loss: 0.725799]\n",
      "epoch: 97, [Discriminator :: d_loss: 1.821134], [ Generator :: loss: 0.738471]\n",
      "epoch: 98, [Discriminator :: d_loss: 2.172983], [ Generator :: loss: 0.742038]\n",
      "epoch: 99, [Discriminator :: d_loss: 1.308688], [ Generator :: loss: 0.707285]\n"
     ]
    }
   ],
   "source": [
    "gan = GAN(X.shape[1])\n",
    "gan.train(X, y, debug=True, epochs=100, batch=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preidiction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = gan.discriminator.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.705"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(y == (predict > 0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[282 118]\n",
      " [  0   0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics \n",
    "print(metrics.confusion_matrix(y, (predict > 0.5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.705"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.accuracy_score(y, (predict > 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=17) \n",
    "cvscores = [] \n",
    "for train, test in kfold.split(X, y): \n",
    "    gan = GAN(X.shape[1]) \n",
    "    gan.train(X[train], y[train], epochs=100, batch=20, debug=False) \n",
    "    predict = gan.discriminator.predict(X[test]) \n",
    "    acc = metrics.f1_score(y[test], (predict > 0.5)) \n",
    "    cvscores.append(acc) \n",
    "    print(acc) \n",
    "print(\"%.2f%% (+/- %.2f%%)\" % (np.mean(cvscores), np.std(cvscores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
