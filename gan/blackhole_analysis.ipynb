{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "import pandas as pd\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.optimizers import Adam, RMSprop, SGD\n",
    "import sys, re, glob\n",
    "import numpy as np\n",
    "from keras.layers import Input, Dense, Flatten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(folder, rng):\n",
    "    X = []\n",
    "\n",
    "    for i in range(rng):\n",
    "\n",
    "        packets = list()\n",
    "        max_node = 0\n",
    "        max_time = 0\n",
    "\n",
    "        for file in glob.glob(\"{}/output_{}/*.routes\".format(folder, i)):\n",
    "\n",
    "            handle = open(file, 'r')\n",
    "            data = handle.read()\n",
    "            handle.close()\n",
    "\n",
    "            _nodes = re.split('\\n\\n', data);\n",
    "\n",
    "            _nodes.pop()\n",
    "            for node in _nodes:\n",
    "\n",
    "                _strs = re.findall('(\\d{1,3}(?:\\.\\d{1,3}){3})\\s+\\d{1,3}(?:\\.\\d{1,3}){3}\\s+\\d{1,3}(?:\\.\\d{1,3}){3}\\s+\\w+\\s+-?\\d+\\.\\d+\\s+(\\d+)', node)\n",
    "\n",
    "                strings = list()\n",
    "                for _str in _strs:\n",
    "                    strings.append(dict(zip(('Destination', 'Hops'), _str)))\n",
    "\n",
    "                header = re.findall('Node:\\s+(\\d+)\\s+Time:\\s+(\\d+)', node)\n",
    "\n",
    "                max_node = max(max_node, int(header[0][0]))\n",
    "                max_time = max(max_time, int(header[0][1]))\n",
    "\n",
    "                for _str in strings:\n",
    "                    _str['Node'] = int(header[0][0])\n",
    "                    _str['Time'] = int(header[0][1])\n",
    "                    packets.append(_str)\n",
    "\n",
    "\n",
    "        table = pd.DataFrame(packets)\n",
    "        time_agg = table.groupby([\"Node\", \"Destination\"]).agg(['min', 'max', \"mean\", \"median\", \"prod\", \"sum\", \"std\", \"var\"])\n",
    "        node_agg = time_agg.groupby(\"Node\").agg(['min', 'max', \"mean\", \"median\", \"prod\", \"sum\", \"std\", \"var\"])\n",
    "        aggregate = node_agg.agg(['min', 'max', \"mean\", \"median\", \"prod\", \"sum\", \"std\", \"var\"])\n",
    "        X.append(aggregate.values.flatten())\n",
    "\n",
    "    X = np.array(X)\n",
    "    X[np.isnan(X) | np.isinf(X)] = 0\n",
    "    \n",
    "    return X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "malicious = load_data(\"../data/malicious\", 100)\n",
    "normal = load_data(\"../data/normal\", 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.clip(0.999, 0.001,np.concatenate((malicious, normal)))\n",
    "y = np.concatenate((np.ones((100, 1)), np.zeros((100, 1))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAN:\n",
    "    def __init__(self, optimizer, shape):\n",
    "        self.OPTIMIZER = optimizer\n",
    "        self.OPTIMIZER = shape\n",
    "        \n",
    "    def generator(self):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(2048, input_shape=(1000,)))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Dense(SHAPE, activation='sigmoid'))\n",
    "        return model\n",
    "    \n",
    "    def generator(self):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(10, input_shape=(SHAPE,)))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "        return model\n",
    "    \n",
    "    def predict(self):\n",
    "        discriminator.trainable = False\n",
    "        model = Sequential()\n",
    "        model.add(generator)\n",
    "        model.add(discriminator)\n",
    "        return model\n",
    "    \n",
    "    def compile_models(self):\n",
    "        generator.compile(loss='binary_crossentropy', optimizer=OPTIMIZER)\n",
    "        discriminator.compile(loss='binary_crossentropy', optimizer=OPTIMIZER, metrics=['accuracy'] )\n",
    "        stacked.compile(loss='binary_crossentropy', optimizer=OPTIMIZER)\n",
    "        return discriminator, generator, stacked\n",
    "    \n",
    "    def train(self, X, y, epochs=200, batch = 10, debug=True):\n",
    "        for cnt in range(epochs):\n",
    "\n",
    "            ## train discriminator\n",
    "            random_index =  np.random.randint(0, len(malicious) - batch)\n",
    "            X_batch = X[random_index : random_index + batch]\n",
    "            y_bacth = y[random_index : random_index + batch]\n",
    "\n",
    "            gen_noise = np.random.normal(0, 1, (batch,1000))\n",
    "            syntetic = generator.predict(gen_noise)\n",
    "\n",
    "            x_combined_batch = np.concatenate((X_batch, syntetic))\n",
    "            y_combined_batch = np.concatenate((y_bacth, np.zeros((batch, 1))))\n",
    "\n",
    "            d_loss = discriminator.train_on_batch(x_combined_batch, y_combined_batch)\n",
    "\n",
    "            # train generator\n",
    "            noise = np.random.normal(0, 1, (batch,1000))\n",
    "            y_mislabled = np.ones((batch, 1))\n",
    "            g_loss = stacked.train_on_batch(noise, y_mislabled)\n",
    "            if (debug):\n",
    "                print ('epoch: %d, [Discriminator :: d_loss: %f], [ Generator :: loss: %f]' % (cnt, d_loss[0], g_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gan = GAN(Adam, malicious.shape[1])\n",
    "gan.train(discriminator, generator, stacked, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preidiction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = discriminator.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.58"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(y == (predict > 0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[88 12]\n",
      " [72 28]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "print(metrics.confusion_matrix(y, (predict > 0.5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.58"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.accuracy_score(y, (predict > 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.55\n",
      "0.65\n",
      "0.65\n",
      "0.55\n",
      "0.55\n",
      "0.7\n",
      "0.45\n",
      "0.65\n",
      "0.5\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=17)\n",
    "cvscores = []\n",
    "for train, test in kfold.split(X, y):\n",
    "    gan = GAN(Adam, malicious.shape[1])\n",
    "    gan.train(X[train], y[train], epochs=10, debug=False)\n",
    "    discriminator, generator, stacked = gan.compile_models()\n",
    "    predict = discriminator.predict(X[test])\n",
    "    acc = metrics.accuracy_score(y[test], (predict > 0.5))\n",
    "    cvscores.append(acc)\n",
    "    print(acc)\n",
    "print(\"%.2f%% (+/- %.2f%%)\" % (np.mean(cvscores), np.std(cvscores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
