{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nsuprotivniy/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "import pandas as pd\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.optimizers import Adam, RMSprop, SGD\n",
    "import sys, re, glob\n",
    "import numpy as np\n",
    "from keras.layers import Input, Dense, Flatten\n",
    "import sys, re, glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from lxml import etree\n",
    "from sklearn.model_selection import StratifiedKFold "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path, rng):\n",
    "    routes = list()\n",
    "    flows = list()\n",
    "\n",
    "    for i in range(rng):\n",
    "\n",
    "        for routes_file in glob.glob('{}/output_{}/*.routes'.format(path, i)):\n",
    "\n",
    "            info = re.findall('(\\w+)/output_(\\d+)', routes_file)\n",
    "\n",
    "            handle = open(routes_file, 'r')\n",
    "            data = handle.read()\n",
    "            handle.close()\n",
    "\n",
    "            nodes = re.split('\\n\\n', data)\n",
    "            nodes.pop()\n",
    "\n",
    "            for node in nodes:\n",
    "\n",
    "                header = re.findall('Node:\\s+(\\d+)\\s+Time:\\s+(\\d+)', node)\n",
    "                lines = re.findall('(\\d{1,3}(?:\\.\\d{1,3}){3})\\s+(\\d{1,3}(?:\\.\\d{1,3}){3})\\s+(\\d{1,3}(?:\\.\\d{1,3}){3})\\s+(\\w+)\\s+(-?\\d+\\.\\d+)\\s+(\\d+)', node)\n",
    "\n",
    "                for line in lines:\n",
    "                    l = list(line)\n",
    "                    l[4] = float(line[4])\n",
    "                    l[5] = int(line[5])\n",
    "                    routes.append(header[0] + tuple(l) + info[0])\n",
    "\n",
    "        for flowmon_file in glob.glob('{}/output_{}/*.flowmon'.format(path, i)):\n",
    " \n",
    "            info = re.findall('(\\w+)/output_(\\d+)', flowmon_file)\n",
    "\n",
    "            with open(flowmon_file) as fobj:\n",
    "                xml = fobj.read()\n",
    "    \n",
    "            root = etree.fromstring(xml)\n",
    "\n",
    "\n",
    "            for flow in root.xpath('/FlowMonitor/FlowStats/Flow'):\n",
    "\n",
    "                attributes = list()\n",
    "\n",
    "                for attrib in flow.attrib:\n",
    "\n",
    "                    attr = flow.attrib[attrib]\n",
    "                    if 'ns' in attr:\n",
    "                        attr = re.findall('(\\d+)', attr)[0]\n",
    "\n",
    "                    attributes.append(int(attr))\n",
    "\n",
    "                flows.append(tuple(attributes) + info[0])\n",
    "\n",
    "    routes_table = pd.DataFrame(routes, columns=['Node', 'Time', 'Destination', 'Gateway', 'Interface', 'Flag', 'Expire', 'Hops', 'Type', 'Test'])\n",
    "\n",
    "    flag_agg = routes_table.groupby(['Type', 'Test', 'Time', 'Flag']).agg({'Flag' : ['count']})\n",
    "    hops_agg = routes_table.groupby(['Type', 'Test', 'Node', 'Destination']).agg({'Hops' : ['min', 'max', 'mean', 'median', 'prod', 'sum', 'std', 'var']})\n",
    "    \n",
    "    unstack_flag_agg = flag_agg.unstack(['Time','Flag']).fillna(0)\n",
    "    unstack_hops_agg = hops_agg.unstack(['Node', 'Destination']).fillna(0)\n",
    "    \n",
    " \n",
    "    flows_table = pd.DataFrame(flows, columns=['flowId', 'timeFirstTxPacket', 'timeFirstRxPacket', 'timeLastTxPacket', 'timeLastRxPacket', 'delaySum', 'jitterSum', 'lastDelay', 'txBytes', 'rxBytes', 'txPackets', 'rxPackets', 'lostPackets', 'timesForwarded', 'Type', 'Test'])\n",
    "\n",
    "    lost_agg = flows_table.groupby(['Type', 'Test']).agg({'lostPackets' : ['sum', 'mean']})\n",
    "    forwarded_agg = flows_table.groupby(['Type', 'Test']).agg({'timesForwarded' : ['sum', 'max', 'mean', 'var']})\n",
    "    \n",
    "    return lost_agg.join(forwarded_agg).join(unstack_flag_agg).join(unstack_hops_agg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nsuprotivniy/anaconda3/lib/python3.6/site-packages/pandas/core/reshape/merge.py:558: UserWarning: merging between different levels can give an unintended result (2 levels on the left, 4 on the right)\n",
      "  warnings.warn(msg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "malicious = load_data(\"../data/malicious\", 100)\n",
    "normal = load_data(\"../data/normal\", 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.concatenate((malicious, normal))\n",
    "y = np.concatenate((np.ones((100, 1)), np.zeros((100, 1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes = np.array(range(len(y)))\n",
    "np.random.shuffle(indexes)\n",
    "indexes\n",
    "X = X[indexes]\n",
    "y = y[indexes]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAN:\n",
    "    def __init__(self, shape):\n",
    "        self.SHAPE = shape\n",
    "        self.OPTIMIZER = Adam()\n",
    "        self.compile_models()\n",
    "        \n",
    "    def __generator(self):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(2048, input_shape=(1000,)))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Dense(self.SHAPE, activation='sigmoid'))\n",
    "        return model\n",
    "    \n",
    "    def __discriminator(self):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(10, input_shape=(self.SHAPE,)))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "        return model\n",
    "    \n",
    "    def __stacked(self, generator, discriminator):\n",
    "        discriminator.trainable = False\n",
    "        model = Sequential()\n",
    "        model.add(generator)\n",
    "        model.add(discriminator)\n",
    "        return model\n",
    "    \n",
    "    def compile_models(self):\n",
    "        self.generator = self.__generator()\n",
    "        self.discriminator = self.__discriminator()\n",
    "        self.stacked = self.__stacked(self.generator, self.discriminator)\n",
    "        \n",
    "        self.generator.compile(loss='binary_crossentropy', optimizer=self.OPTIMIZER)\n",
    "        self.discriminator.compile(loss='binary_crossentropy', optimizer=self.OPTIMIZER, metrics=['accuracy'] )\n",
    "        self.stacked.compile(loss='binary_crossentropy', optimizer=self.OPTIMIZER)\n",
    "        \n",
    "    \n",
    "    def train(self,X, y, epochs=200, batch = 100, debug=False):\n",
    "        for cnt in range(epochs):\n",
    "\n",
    "            ## train discriminator\n",
    "            random_index =  np.random.randint(0, len(y) - batch)\n",
    "            X_batch = X[random_index : random_index + batch]\n",
    "            y_batch = y[random_index : random_index + batch]\n",
    "\n",
    "            gen_noise = np.random.normal(0, 1, (batch,1000))\n",
    "            syntetic = self.generator.predict(gen_noise)\n",
    "                \n",
    "            x_combined_batch = np.concatenate((X_batch, syntetic))\n",
    "            y_combined_batch = np.concatenate((y_batch, np.zeros((batch, 1))))\n",
    "\n",
    "            d_loss = self.discriminator.train_on_batch(x_combined_batch, y_combined_batch)\n",
    "\n",
    "            # train generator\n",
    "            noise = np.random.normal(0, 1, (batch,1000))\n",
    "            y_mislabled = np.ones((batch, 1))\n",
    "            g_loss = self.stacked.train_on_batch(noise, y_mislabled)\n",
    "            if debug:\n",
    "                print ('epoch: %d, [Discriminator :: d_loss: %f], [ Generator :: loss: %f]' % (cnt, d_loss[0], g_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, [Discriminator :: d_loss: 4.564472], [ Generator :: loss: 0.879590]\n",
      "epoch: 1, [Discriminator :: d_loss: 4.517925], [ Generator :: loss: 0.998926]\n",
      "epoch: 2, [Discriminator :: d_loss: 4.490653], [ Generator :: loss: 1.124255]\n",
      "epoch: 3, [Discriminator :: d_loss: 4.501699], [ Generator :: loss: 1.206633]\n",
      "epoch: 4, [Discriminator :: d_loss: 4.498006], [ Generator :: loss: 1.189893]\n",
      "epoch: 5, [Discriminator :: d_loss: 4.496871], [ Generator :: loss: 1.244746]\n",
      "epoch: 6, [Discriminator :: d_loss: 4.504513], [ Generator :: loss: 1.250917]\n",
      "epoch: 7, [Discriminator :: d_loss: 4.504346], [ Generator :: loss: 1.284298]\n",
      "epoch: 8, [Discriminator :: d_loss: 4.507780], [ Generator :: loss: 1.329013]\n",
      "epoch: 9, [Discriminator :: d_loss: 4.506391], [ Generator :: loss: 1.267786]\n",
      "epoch: 10, [Discriminator :: d_loss: 4.481155], [ Generator :: loss: 1.284520]\n",
      "epoch: 11, [Discriminator :: d_loss: 4.536175], [ Generator :: loss: 1.306473]\n",
      "epoch: 12, [Discriminator :: d_loss: 4.522892], [ Generator :: loss: 1.310297]\n",
      "epoch: 13, [Discriminator :: d_loss: 4.529692], [ Generator :: loss: 1.222757]\n",
      "epoch: 14, [Discriminator :: d_loss: 4.548321], [ Generator :: loss: 1.260048]\n",
      "epoch: 15, [Discriminator :: d_loss: 4.536980], [ Generator :: loss: 1.231060]\n",
      "epoch: 16, [Discriminator :: d_loss: 4.558184], [ Generator :: loss: 1.195748]\n",
      "epoch: 17, [Discriminator :: d_loss: 4.564573], [ Generator :: loss: 1.149351]\n",
      "epoch: 18, [Discriminator :: d_loss: 4.621721], [ Generator :: loss: 1.041199]\n",
      "epoch: 19, [Discriminator :: d_loss: 4.598853], [ Generator :: loss: 0.997507]\n",
      "epoch: 20, [Discriminator :: d_loss: 4.717660], [ Generator :: loss: 0.931957]\n",
      "epoch: 21, [Discriminator :: d_loss: 4.721557], [ Generator :: loss: 0.880944]\n",
      "epoch: 22, [Discriminator :: d_loss: 4.778588], [ Generator :: loss: 0.868080]\n",
      "epoch: 23, [Discriminator :: d_loss: 4.943413], [ Generator :: loss: 0.780097]\n",
      "epoch: 24, [Discriminator :: d_loss: 5.078357], [ Generator :: loss: 0.725814]\n",
      "epoch: 25, [Discriminator :: d_loss: 5.203509], [ Generator :: loss: 0.693167]\n",
      "epoch: 26, [Discriminator :: d_loss: 5.247588], [ Generator :: loss: 0.626099]\n",
      "epoch: 27, [Discriminator :: d_loss: 5.385360], [ Generator :: loss: 0.628993]\n",
      "epoch: 28, [Discriminator :: d_loss: 5.383682], [ Generator :: loss: 0.571679]\n",
      "epoch: 29, [Discriminator :: d_loss: 5.370149], [ Generator :: loss: 0.526500]\n",
      "epoch: 30, [Discriminator :: d_loss: 5.552612], [ Generator :: loss: 0.496536]\n",
      "epoch: 31, [Discriminator :: d_loss: 5.669190], [ Generator :: loss: 0.477928]\n",
      "epoch: 32, [Discriminator :: d_loss: 5.825240], [ Generator :: loss: 0.431379]\n",
      "epoch: 33, [Discriminator :: d_loss: 5.831153], [ Generator :: loss: 0.466284]\n",
      "epoch: 34, [Discriminator :: d_loss: 6.132441], [ Generator :: loss: 0.464831]\n",
      "epoch: 35, [Discriminator :: d_loss: 6.345784], [ Generator :: loss: 0.390419]\n",
      "epoch: 36, [Discriminator :: d_loss: 6.238867], [ Generator :: loss: 0.426546]\n",
      "epoch: 37, [Discriminator :: d_loss: 6.168301], [ Generator :: loss: 0.330000]\n",
      "epoch: 38, [Discriminator :: d_loss: 6.427639], [ Generator :: loss: 0.433868]\n",
      "epoch: 39, [Discriminator :: d_loss: 6.666513], [ Generator :: loss: 0.383761]\n",
      "epoch: 40, [Discriminator :: d_loss: 6.827992], [ Generator :: loss: 0.338251]\n",
      "epoch: 41, [Discriminator :: d_loss: 6.920947], [ Generator :: loss: 0.311643]\n",
      "epoch: 42, [Discriminator :: d_loss: 6.673489], [ Generator :: loss: 0.324794]\n",
      "epoch: 43, [Discriminator :: d_loss: 6.946951], [ Generator :: loss: 0.335399]\n",
      "epoch: 44, [Discriminator :: d_loss: 7.091256], [ Generator :: loss: 0.226849]\n",
      "epoch: 45, [Discriminator :: d_loss: 7.073454], [ Generator :: loss: 0.307548]\n",
      "epoch: 46, [Discriminator :: d_loss: 7.173709], [ Generator :: loss: 0.307922]\n",
      "epoch: 47, [Discriminator :: d_loss: 7.289443], [ Generator :: loss: 0.253506]\n",
      "epoch: 48, [Discriminator :: d_loss: 7.022604], [ Generator :: loss: 0.357403]\n",
      "epoch: 49, [Discriminator :: d_loss: 7.445195], [ Generator :: loss: 0.290521]\n",
      "epoch: 50, [Discriminator :: d_loss: 7.488863], [ Generator :: loss: 0.264452]\n",
      "epoch: 51, [Discriminator :: d_loss: 7.593384], [ Generator :: loss: 0.195063]\n",
      "epoch: 52, [Discriminator :: d_loss: 7.506176], [ Generator :: loss: 0.167661]\n",
      "epoch: 53, [Discriminator :: d_loss: 7.705866], [ Generator :: loss: 0.143282]\n",
      "epoch: 54, [Discriminator :: d_loss: 7.461204], [ Generator :: loss: 0.252880]\n",
      "epoch: 55, [Discriminator :: d_loss: 7.640145], [ Generator :: loss: 0.329512]\n",
      "epoch: 56, [Discriminator :: d_loss: 7.894735], [ Generator :: loss: 0.202965]\n",
      "epoch: 57, [Discriminator :: d_loss: 7.789955], [ Generator :: loss: 0.129303]\n",
      "epoch: 58, [Discriminator :: d_loss: 8.274441], [ Generator :: loss: 0.168834]\n",
      "epoch: 59, [Discriminator :: d_loss: 7.971894], [ Generator :: loss: 0.158287]\n",
      "epoch: 60, [Discriminator :: d_loss: 8.312363], [ Generator :: loss: 0.217103]\n",
      "epoch: 61, [Discriminator :: d_loss: 8.463853], [ Generator :: loss: 0.121913]\n",
      "epoch: 62, [Discriminator :: d_loss: 8.333028], [ Generator :: loss: 0.089759]\n",
      "epoch: 63, [Discriminator :: d_loss: 8.117419], [ Generator :: loss: 0.171618]\n",
      "epoch: 64, [Discriminator :: d_loss: 8.212401], [ Generator :: loss: 0.283654]\n",
      "epoch: 65, [Discriminator :: d_loss: 8.224095], [ Generator :: loss: 0.353997]\n",
      "epoch: 66, [Discriminator :: d_loss: 8.374366], [ Generator :: loss: 0.175268]\n",
      "epoch: 67, [Discriminator :: d_loss: 8.478669], [ Generator :: loss: 0.094725]\n",
      "epoch: 68, [Discriminator :: d_loss: 8.411589], [ Generator :: loss: 0.169677]\n",
      "epoch: 69, [Discriminator :: d_loss: 8.495152], [ Generator :: loss: 0.215551]\n",
      "epoch: 70, [Discriminator :: d_loss: 8.711626], [ Generator :: loss: 0.184085]\n",
      "epoch: 71, [Discriminator :: d_loss: 8.605458], [ Generator :: loss: 0.186291]\n",
      "epoch: 72, [Discriminator :: d_loss: 8.797685], [ Generator :: loss: 0.229257]\n",
      "epoch: 73, [Discriminator :: d_loss: 8.712350], [ Generator :: loss: 0.192214]\n",
      "epoch: 74, [Discriminator :: d_loss: 8.768814], [ Generator :: loss: 0.215483]\n",
      "epoch: 75, [Discriminator :: d_loss: 8.541109], [ Generator :: loss: 0.070000]\n",
      "epoch: 76, [Discriminator :: d_loss: 8.511174], [ Generator :: loss: 0.125457]\n",
      "epoch: 77, [Discriminator :: d_loss: 8.626789], [ Generator :: loss: 0.075266]\n",
      "epoch: 78, [Discriminator :: d_loss: 8.497097], [ Generator :: loss: 0.108478]\n",
      "epoch: 79, [Discriminator :: d_loss: 8.900842], [ Generator :: loss: 0.144476]\n",
      "epoch: 80, [Discriminator :: d_loss: 8.924701], [ Generator :: loss: 0.178023]\n",
      "epoch: 81, [Discriminator :: d_loss: 8.737425], [ Generator :: loss: 0.214786]\n",
      "epoch: 82, [Discriminator :: d_loss: 8.917032], [ Generator :: loss: 0.097841]\n",
      "epoch: 83, [Discriminator :: d_loss: 9.041886], [ Generator :: loss: 0.117308]\n",
      "epoch: 84, [Discriminator :: d_loss: 8.916135], [ Generator :: loss: 0.165476]\n",
      "epoch: 85, [Discriminator :: d_loss: 9.111509], [ Generator :: loss: 0.121824]\n",
      "epoch: 86, [Discriminator :: d_loss: 8.773710], [ Generator :: loss: 0.054709]\n",
      "epoch: 87, [Discriminator :: d_loss: 8.960297], [ Generator :: loss: 0.120859]\n",
      "epoch: 88, [Discriminator :: d_loss: 8.997198], [ Generator :: loss: 0.225225]\n",
      "epoch: 89, [Discriminator :: d_loss: 9.034499], [ Generator :: loss: 0.248615]\n",
      "epoch: 90, [Discriminator :: d_loss: 9.154554], [ Generator :: loss: 0.057772]\n",
      "epoch: 91, [Discriminator :: d_loss: 9.177683], [ Generator :: loss: 0.079584]\n",
      "epoch: 92, [Discriminator :: d_loss: 9.243465], [ Generator :: loss: 0.081463]\n",
      "epoch: 93, [Discriminator :: d_loss: 9.256473], [ Generator :: loss: 0.260516]\n",
      "epoch: 94, [Discriminator :: d_loss: 9.367756], [ Generator :: loss: 0.148920]\n",
      "epoch: 95, [Discriminator :: d_loss: 9.154618], [ Generator :: loss: 0.188750]\n",
      "epoch: 96, [Discriminator :: d_loss: 9.185100], [ Generator :: loss: 0.091034]\n",
      "epoch: 97, [Discriminator :: d_loss: 9.255010], [ Generator :: loss: 0.072243]\n",
      "epoch: 98, [Discriminator :: d_loss: 9.226716], [ Generator :: loss: 0.180568]\n",
      "epoch: 99, [Discriminator :: d_loss: 9.427092], [ Generator :: loss: 0.082802]\n"
     ]
    }
   ],
   "source": [
    "gan = GAN(malicious.shape[1])\n",
    "gan.train(X, y, debug=True, epochs=100, batch=199)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preidiction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = gan.discriminator.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.47"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(y == (predict > 0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[78 22]\n",
      " [84 16]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics \n",
    "print(metrics.confusion_matrix(y, (predict > 0.5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.47"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.accuracy_score(y, (predict > 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4\n",
      "0.55\n",
      "0.4\n",
      "0.65\n",
      "0.5\n",
      "0.35\n",
      "0.45\n",
      "0.35\n",
      "0.45\n",
      "0.35\n",
      "0.44% (+/- 0.09%)\n"
     ]
    }
   ],
   "source": [
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=17) \n",
    "cvscores = [] \n",
    "for train, test in kfold.split(X, y): \n",
    "    gan = GAN(malicious.shape[1]) \n",
    "    gan.train(X[train], y[train], epochs=100, batch=100, debug=False) \n",
    "    predict = gan.discriminator.predict(X[test]) \n",
    "    acc = metrics.accuracy_score(y[test], (predict > 0.5)) \n",
    "    cvscores.append(acc) \n",
    "    print(acc) \n",
    "print(\"%.2f%% (+/- %.2f%%)\" % (np.mean(cvscores), np.std(cvscores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
