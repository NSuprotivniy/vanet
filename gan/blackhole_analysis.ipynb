{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys, os, pickle\n",
    "from keras.models import Sequential\n",
    "import pandas as pd\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.optimizers import Adam, RMSprop, SGD\n",
    "import sys, re, glob\n",
    "import numpy as np\n",
    "from keras.layers import Input, Dense, Flatten\n",
    "import sys, re, glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from lxml import etree\n",
    "from sklearn.model_selection import StratifiedKFold "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "proto_dict = {\n",
    "    'ns3::WifiMacHeader' : 'MAC',\n",
    "    'ns3::LlcSnapHeader' : 'LLC',\n",
    "    'ns3::ArpHeader' : 'ARP',\n",
    "    'ns3::Ipv4Header' : 'IPv4',\n",
    "    'ns3::UdpHeader' : 'UDP',\n",
    "    'ns3::aodv::TypeHeader' : 'AODV_Type',\n",
    "    'ns3::aodv::RrepHeader' : 'AODV_RREP',\n",
    "    'ns3::aodv::RreqHeader' : 'AODV_RREQ',\n",
    "    'ns3::aodv::RrepAckHeader' : 'AODV_RACK',\n",
    "    'ns3::aodv::RerrHeader' : 'AODV_RERR',\n",
    "    'ns3::Icmpv4Header' : 'ICMPv4_HEADER',\n",
    "    'ns3::Icmpv4TimeExceeded' : 'ICMPv4_TE',\n",
    "    'ns3::Icmpv4DestinationUnreachable' : 'ICMPv4_DU'\n",
    "}\n",
    "\n",
    "def parce_packets(input_file):\n",
    "    with open(input_file, 'r') as f:\n",
    "        data = f.read()\n",
    "    data = data.split('meta-info=\"')\n",
    "    meta_info = []\n",
    "    data.pop(0)\n",
    "    for b in data:\n",
    "        meta_info.append(b.split('\"')[0])\n",
    "    packets = []\n",
    "    for m in meta_info:\n",
    "        raw = {}\n",
    "        for proto in proto_dict:\n",
    "            if m.find(proto) != -1:\n",
    "                raw[proto_dict[proto]] = m.split(proto+' (')[1].split(') ns3')[0]\n",
    "        p = {}\n",
    "        if 'MAC' in raw and 'LLC' in raw:\n",
    "            p['MAC'] = {}\n",
    "            p['MAC']['src'] = raw['MAC'].split('SA=')[1].split(',')[0]\n",
    "            p['MAC']['dst'] = raw['MAC'].split('DA=')[1].split(',')[0]\n",
    "            p['MAC']['type'] = int(raw['LLC'].split('type ')[1], 16)\n",
    "        if 'ARP' in raw:\n",
    "            p['ARP'] = {}\n",
    "            if raw['ARP'].find('request') != -1:\n",
    "                p['ARP']['type'] = 'request'\n",
    "            else:\n",
    "                p['ARP']['type'] = 'reply'\n",
    "                p['ARP']['MAC_dst'] = raw['ARP'].split('dest mac: ')[1].split(' ')[0]\n",
    "            p['ARP']['MAC_src'] = raw['ARP'].split('source mac: ')[1].split(' ')[0]\n",
    "            p['ARP']['IP_src'] = raw['ARP'].split('source ipv4: ')[1].split(' ')[0]\n",
    "            p['ARP']['IP_dst'] = raw['ARP'].split('dest ipv4: ')[1]\n",
    "        if 'IPv4' in raw:\n",
    "            p['IPv4'] = {}\n",
    "            ipv4data = raw['IPv4'].split(' ')\n",
    "            p['IPv4']['src'] = ipv4data[-3]\n",
    "            p['IPv4']['dst'] = ipv4data[-1]\n",
    "            p['IPv4']['proto'] = int(raw['IPv4'].split('protocol ')[1].split(' ')[0])\n",
    "            p['IPv4']['ttl'] = int(raw['IPv4'].split('ttl ')[1].split(' ')[0])\n",
    "            p['IPv4']['length'] = int(raw['IPv4'].split('length: ')[1].split(' ')[0])\n",
    "        if 'UDP' in raw:\n",
    "            p['UDP'] = {}\n",
    "            p['UDP']['length'] = int(raw['UDP'].split('length: ')[1].split(' ')[0])\n",
    "        if 'AODV_Type' in raw:\n",
    "            p['AODV'] = {}\n",
    "            p['AODV']['type'] = raw['AODV_Type']\n",
    "        if 'AODV_RREP' in raw:\n",
    "            p['AODV']['RREP'] = {}\n",
    "            p['AODV']['RREP']['dst'] = raw['AODV_RREP'].split('destination: ipv4 ')[1].split(' ')[0]\n",
    "            p['AODV']['RREP']['src'] = raw['AODV_RREP'].split('source ipv4 ')[1].split(' ')[0]\n",
    "            p['AODV']['RREP']['SN'] = int(raw['AODV_RREP'].split('sequence number ')[1].split(' ')[0])\n",
    "            p['AODV']['RREP']['lifetime'] = int(raw['AODV_RREP'].split('lifetime ')[1].split(' ')[0])\n",
    "            p['AODV']['RREP']['acknowledgment'] = raw['AODV_RREP'].split('acknowledgment ')[1].split(' ')[0]\n",
    "            p['AODV']['RREP']['flag'] = raw['AODV_RREP'].split('flag ')[1]\n",
    "        if 'AODV_RREQ' in raw:\n",
    "            p['AODV']['RREQ'] = {}\n",
    "            p['AODV']['RREQ']['ID'] = int(raw['AODV_RREQ'].split('ID ')[1].split(' ')[0])\n",
    "            p['AODV']['RREQ']['dst'] = raw['AODV_RREQ'].split('destination: ipv4 ')[1].split(' ')[0]\n",
    "            p['AODV']['RREQ']['src'] = raw['AODV_RREQ'].split('source: ipv4 ')[1].split(' ')[0]\n",
    "            p['AODV']['RREQ']['SN'] = int(raw['AODV_RREQ'].split('sequence number ')[1].split(' ')[0])\n",
    "            p['AODV']['RREQ']['flags'] = raw['AODV_RREQ'].split('flags: ')[1]\n",
    "        if 'AODV_RACK' in raw:\n",
    "            pass # seems to be empty\n",
    "        if 'AODV_RERR' in raw:\n",
    "            p['AODV']['RERR'] = raw['AODV_RERR']\n",
    "        if 'ICMPv4_HEADER' in raw:\n",
    "            p['ICMPv4'] = {}\n",
    "            p['ICMPv4']['type'] = int(raw['ICMPv4_HEADER'].split('type=')[1].split(',')[0])\n",
    "            p['ICMPv4']['code'] = int(raw['ICMPv4_HEADER'].split('code=')[1])\n",
    "        if 'ICMPv4_TE' in raw:\n",
    "            p['ICMPv4']['data'] = raw['ICMPv4_TE']\n",
    "        if 'ICMPv4_DU' in raw:\n",
    "            p['ICMPv4']['data'] = raw['ICMPv4_DU']\n",
    "\n",
    "        if len(p) > 0:\n",
    "            packets.append(p)\n",
    "    return packets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mac_to_id(mac):\n",
    "    return int(mac.split(':')[-1], 16) - 1\n",
    "\n",
    "def ip_to_id(ip):\n",
    "    return int(ip.split('.')[-1]) - 1\n",
    "\n",
    "def entropy(arr):\n",
    "    s = arr.sum()\n",
    "    arr = arr / s\n",
    "    arr = arr * np.log(arr) / np.log(2)\n",
    "    return -(arr.sum() / np.log(len(arr)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path, rng):\n",
    "    routes = list()\n",
    "    flows = list()\n",
    "    packets = list()\n",
    "\n",
    "    for i in range(rng):\n",
    "\n",
    "        for routes_file in glob.glob('{}/output_{}/*.routes'.format(path, i)):\n",
    "\n",
    "            info = re.findall('(\\w+)/output_(\\d+)', routes_file)\n",
    "\n",
    "            handle = open(routes_file, 'r')\n",
    "            data = handle.read()\n",
    "            handle.close()\n",
    "\n",
    "            nodes = re.split('\\n\\n', data)\n",
    "            nodes.pop()\n",
    "\n",
    "            for node in nodes:\n",
    "\n",
    "                header = re.findall('Node:\\s+(\\d+)\\s+Time:\\s+(\\d+)', node)\n",
    "                lines = re.findall('(\\d{1,3}(?:\\.\\d{1,3}){3})\\s+(\\d{1,3}(?:\\.\\d{1,3}){3})\\s+(\\d{1,3}(?:\\.\\d{1,3}){3})\\s+(\\w+)\\s+(-?\\d+\\.\\d+)\\s+(\\d+)', node)\n",
    "\n",
    "                for line in lines:\n",
    "                    l = list(line)\n",
    "                    l[4] = float(line[4])\n",
    "                    l[5] = int(line[5])\n",
    "                    routes.append(header[0] + tuple(l) + info[0])\n",
    "\n",
    "        for flowmon_file in glob.glob('{}/output_{}/*.flowmon'.format(path, i)):\n",
    "\n",
    "            info = re.findall('(\\w+)/output_(\\d+)', flowmon_file)\n",
    "\n",
    "            with open(flowmon_file) as fobj:\n",
    "                xml = fobj.read()\n",
    "\n",
    "            root = etree.fromstring(xml)\n",
    "\n",
    "\n",
    "            for flow in root.xpath('/FlowMonitor/FlowStats/Flow'):\n",
    "\n",
    "                attributes = list()\n",
    "\n",
    "                for attrib in flow.attrib:\n",
    "\n",
    "                    attr = flow.attrib[attrib]\n",
    "                    if 'ns' in attr:\n",
    "                        attr = re.findall('(\\d+)', attr)[0]\n",
    "\n",
    "                    attributes.append(int(attr))\n",
    "\n",
    "                flows.append(tuple(attributes) + info[0])\n",
    "\n",
    "        for packets_file in glob.glob('{}/output_{}/*.xml'.format(path, i)):\n",
    "\n",
    "            if \"routingtable-wireless.xml\" in packets_file:\n",
    "                continue\n",
    "\n",
    "            info = re.findall('(\\w+)/output_(\\d+)', packets_file)\n",
    "\n",
    "            packets_dict = parce_packets(packets_file)\n",
    "            \n",
    "            table_packets = []\n",
    "            for p in packets_dict:\n",
    "                if 'IPv4' in p:\n",
    "                    table_packets.append([mac_to_id(p['MAC']['src']), mac_to_id(p['MAC']['dst']), ip_to_id(p['IPv4']['src']), ip_to_id(p['IPv4']['dst'])])\n",
    "\n",
    "            table_packets = pd.DataFrame(table_packets, columns=['mac_src', 'mac_dst', 'ip_src', 'ip_dst'])\n",
    "            table_packets = table_packets[table_packets['ip_dst'] != 254]\n",
    "            table_packets['input'] = 0\n",
    "            table_packets['output'] = 0\n",
    "            input_cnt = table_packets.groupby('mac_src').agg({'input':'count'})\n",
    "            output_cnt = table_packets.groupby('mac_dst').agg({'output':'count'})\n",
    "            io_mac = input_cnt.join(output_cnt)\n",
    "            io_mac[\"diff\"] = io_mac['input'] - io_mac['output']\n",
    "            io_mac[\"normalized\"] = io_mac[\"diff\"] - io_mac[\"diff\"].min() + 1\n",
    "            \n",
    "            input = io_mac.agg({'input': [entropy, 'min', 'max', 'mean', 'var']}).values.flatten()\n",
    "            output = io_mac.agg({'output': [entropy, 'min', 'max', 'mean', 'var']}).values.flatten()\n",
    "            diff = io_mac.agg({'diff': ['min', 'max', 'mean', 'var']}).values.flatten()\n",
    "            normalized = io_mac.agg({'normalized': [entropy, 'min', 'max', 'mean', 'var']}).values.flatten()\n",
    "            src_count = table_packets[table_packets['mac_src'] == table_packets['ip_src']].count()[0]\n",
    "            dst_count = table_packets[table_packets['mac_dst'] == table_packets['ip_dst']].count()[0]\n",
    "\n",
    "            packets.append(np.concatenate((input, output, diff, normalized, [src_count, dst_count], info[0])))\n",
    "\n",
    "\n",
    "    routes_table = pd.DataFrame(routes, columns=['Node', 'Time', 'Destination', 'Gateway', 'Interface', 'Flag', 'Expire', 'Hops', 'Type', 'Test'])\n",
    "\n",
    "    #print(routes_table)\n",
    "    \n",
    "    flag_count = routes_table.groupby(['Type', 'Test', 'Time', 'Flag']).agg({'Flag' : ['count']})\n",
    "    flag_count = flag_count.reset_index(col_level=1)\n",
    "    flag_count.columns = flag_count.columns.droplevel()\n",
    "    flag_agg = flag_count.groupby(['Type', 'Test', 'Flag']).agg({'count': ['max', 'min', 'mean', 'var']}).unstack().reset_index()\n",
    "    flag_agg.columns = [col[0]+col[1]+col[2] for col in flag_agg.columns]\n",
    "    flag_agg = flag_agg.set_index(['Type', 'Test'])\n",
    "\n",
    "    hops_node_dest_agg = routes_table.groupby(['Type', 'Test', 'Node', 'Destination']).agg({'Hops' : ['min', 'max', 'mean']})\n",
    "    hops_node_agg = hops_node_dest_agg.reset_index().groupby([\"Type\", \"Test\", \"Node\"]).agg(['min', 'max', 'mean']).reset_index(col_level=1)\n",
    "    hops_node_agg.columns = hops_node_agg.columns.droplevel()\n",
    "    hops_agg = hops_node_agg.reset_index().groupby([\"Type\", \"Test\"]).agg(['min', 'max', 'mean']).reset_index(col_level=1)\n",
    "    hops_agg.columns = [col[0]+col[1]+col[2] for col in hops_agg.columns]\n",
    "    hops_agg = hops_agg.set_index(['Type', 'Test'])\n",
    "\n",
    "    flows_table = pd.DataFrame(flows, columns=['flowId', 'timeFirstTxPacket', 'timeFirstRxPacket', 'timeLastTxPacket', 'timeLastRxPacket', 'delaySum', 'jitterSum', 'lastDelay', 'txBytes', 'rxBytes', 'txPackets', 'rxPackets', 'lostPackets', 'timesForwarded', 'Type', 'Test'])\n",
    "\n",
    "    lost_agg = flows_table.groupby(['Type', 'Test']).agg({'lostPackets' : ['sum', 'mean']})\n",
    "    forwarded_agg = flows_table.groupby(['Type', 'Test']).agg({'timesForwarded' : ['sum', 'max', 'mean', 'var']})\n",
    "\n",
    "    packets_table = pd.DataFrame(packets, columns=[\"entopy_input\", \"min_input\", \"max_input\", \"mean_input\", \"var_input\", \"entopy_output\", \"min_output\", \"max_output\", \"mean_output\", \"var_output\", \"min_diff\", \"max_diff\", \"mean_diff\", \"var_diff\", \"entopy_normolized\", \"min_normolized\", \"max_normolized\", \"mean_normolized\", \"var_normolized\", \"src_count\", \"dst_count\", 'Type', 'Test'])\n",
    "    packets_table = packets_table.set_index(['Type', 'Test'])\n",
    "\n",
    "    return flag_agg.join(hops_agg).join(lost_agg).join(forwarded_agg).join(packets_table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\mrale\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\pandas\\core\\generic.py:3111: PerformanceWarning: dropping on a non-lexsorted multi-index without a level parameter may impact performance.\n",
      "  obj = obj._drop_axis(labels, axis, level=level, errors=errors)\n",
      "c:\\users\\mrale\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\pandas\\core\\reshape\\merge.py:544: UserWarning: merging between different levels can give an unintended result (1 levels on the left, 2 on the right)\n",
      "  warnings.warn(msg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "normal = load_data(\"../../../normal\", 100)\n",
    "blackhole = load_data(\"../../../blackhole\", 100)\n",
    "grayhole = load_data(\"../../../greyhole\", 100)\n",
    "wormhole = load_data(\"../../../wormhole\", 100)\n",
    "ddos = load_data(\"../../../ddos\", 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.concatenate((normal, blackhole, grayhole, ddos)).astype(np.float)\n",
    "y = np.concatenate((np.zeros((100, 1)), np.ones((300, 1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes = np.array(range(len(y)))\n",
    "np.random.shuffle(indexes)\n",
    "indexes\n",
    "X = X[indexes]\n",
    "y = y[indexes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[np.isnan(X)] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAN:\n",
    "    def __init__(self, shape):\n",
    "        self.SHAPE = shape\n",
    "        self.OPTIMIZER = Adam()\n",
    "        self.compile_models()\n",
    "        \n",
    "    def __generator(self):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(2048, input_shape=(1000,)))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dense(1024))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dense(512))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dense(256))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dense(self.SHAPE, activation='sigmoid'))\n",
    "        #model = Sequential()\n",
    "        #model.add(Dense(2048, input_shape=(1000,)))\n",
    "        #model.add(Dense(4096))\n",
    "        #model.add(Dense(4096))\n",
    "        #model.add(Dense(4096))\n",
    "        #model.add(Dense(4096))\n",
    "        #model.add(Dense(4096))\n",
    "        #model.add(Dense(4096))\n",
    "        #model.add(Dense(self.SHAPE, activation='sigmoid'))\n",
    "        return model\n",
    "    \n",
    "    def __discriminator(self):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(10, input_shape=(self.SHAPE,)))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Dense(256))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Dense(512))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Dense(1024))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "        #model = Sequential()\n",
    "        #model.add(Dense(10, input_shape=(self.SHAPE,)))\n",
    "        #model.add(Dense(4096))\n",
    "        #model.add(Dense(4096))\n",
    "        #model.add(Dense(4096))\n",
    "        #model.add(Dense(4096))\n",
    "        #model.add(Dense(4096))\n",
    "        #model.add(Dense(4096))\n",
    "        #model.add(Dense(1, activation='sigmoid'))\n",
    "        return model\n",
    "    \n",
    "    def __stacked(self, generator, discriminator):\n",
    "        discriminator.trainable = False\n",
    "        model = Sequential()\n",
    "        model.add(generator)\n",
    "        model.add(discriminator)\n",
    "        return model\n",
    "    \n",
    "    def compile_models(self):\n",
    "        self.generator = self.__generator()\n",
    "        self.discriminator = self.__discriminator()\n",
    "        self.stacked = self.__stacked(self.generator, self.discriminator)\n",
    "        \n",
    "        self.generator.compile(loss='binary_crossentropy', optimizer=self.OPTIMIZER)\n",
    "        self.discriminator.compile(loss='binary_crossentropy', optimizer=self.OPTIMIZER, metrics=['accuracy'] )\n",
    "        self.stacked.compile(loss='binary_crossentropy', optimizer=self.OPTIMIZER)\n",
    "        \n",
    "    \n",
    "    def train(self,X, y, epochs=200, batch = 100, debug=False):\n",
    "        for cnt in range(epochs):\n",
    "\n",
    "            ## train discriminator\n",
    "            random_index =  np.random.randint(0, len(y) - batch)\n",
    "            X_batch = X[random_index : random_index + batch]\n",
    "            y_batch = y[random_index : random_index + batch]\n",
    "\n",
    "            gen_noise = np.random.normal(0, 1, (batch,1000))\n",
    "            syntetic = self.generator.predict(gen_noise)\n",
    "                \n",
    "            x_combined_batch = np.concatenate((X_batch, syntetic))\n",
    "            y_combined_batch = np.concatenate((y_batch, np.zeros((batch, 1))))\n",
    "\n",
    "            d_loss = self.discriminator.train_on_batch(x_combined_batch, y_combined_batch)\n",
    "\n",
    "            # train generator\n",
    "            noise = np.random.normal(0, 1, (batch,1000))\n",
    "            y_mislabled = np.ones((batch, 1))\n",
    "            g_loss = self.stacked.train_on_batch(noise, y_mislabled)\n",
    "            if debug:\n",
    "                print ('epoch: %d, [Discriminator :: d_loss: %f], [ Generator :: loss: %f]' % (cnt, d_loss[0], g_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, [Discriminator :: d_loss: 0.758606], [ Generator :: loss: 0.901949]\n",
      "epoch: 1, [Discriminator :: d_loss: 0.664362], [ Generator :: loss: 0.789370]\n",
      "epoch: 2, [Discriminator :: d_loss: 0.512109], [ Generator :: loss: 0.799172]\n",
      "epoch: 3, [Discriminator :: d_loss: 0.582593], [ Generator :: loss: 0.822038]\n",
      "epoch: 4, [Discriminator :: d_loss: 0.585304], [ Generator :: loss: 0.856298]\n",
      "epoch: 5, [Discriminator :: d_loss: 0.766437], [ Generator :: loss: 0.827429]\n",
      "epoch: 6, [Discriminator :: d_loss: 0.767654], [ Generator :: loss: 0.908840]\n",
      "epoch: 7, [Discriminator :: d_loss: 0.548322], [ Generator :: loss: 0.861684]\n",
      "epoch: 8, [Discriminator :: d_loss: 0.603261], [ Generator :: loss: 0.841199]\n",
      "epoch: 9, [Discriminator :: d_loss: 0.682491], [ Generator :: loss: 0.852156]\n",
      "epoch: 10, [Discriminator :: d_loss: 0.778345], [ Generator :: loss: 0.746033]\n",
      "epoch: 11, [Discriminator :: d_loss: 0.898617], [ Generator :: loss: 0.832004]\n",
      "epoch: 12, [Discriminator :: d_loss: 0.943781], [ Generator :: loss: 0.853764]\n",
      "epoch: 13, [Discriminator :: d_loss: 0.776114], [ Generator :: loss: 0.779537]\n",
      "epoch: 14, [Discriminator :: d_loss: 0.641903], [ Generator :: loss: 0.808106]\n",
      "epoch: 15, [Discriminator :: d_loss: 0.525117], [ Generator :: loss: 0.780662]\n",
      "epoch: 16, [Discriminator :: d_loss: 1.056944], [ Generator :: loss: 0.782843]\n",
      "epoch: 17, [Discriminator :: d_loss: 0.759077], [ Generator :: loss: 0.832146]\n",
      "epoch: 18, [Discriminator :: d_loss: 0.707457], [ Generator :: loss: 0.831013]\n",
      "epoch: 19, [Discriminator :: d_loss: 0.466659], [ Generator :: loss: 0.744681]\n",
      "epoch: 20, [Discriminator :: d_loss: 0.518296], [ Generator :: loss: 0.784031]\n",
      "epoch: 21, [Discriminator :: d_loss: 0.730305], [ Generator :: loss: 0.786274]\n",
      "epoch: 22, [Discriminator :: d_loss: 0.610289], [ Generator :: loss: 0.765860]\n",
      "epoch: 23, [Discriminator :: d_loss: 0.657539], [ Generator :: loss: 0.769159]\n",
      "epoch: 24, [Discriminator :: d_loss: 0.546894], [ Generator :: loss: 0.796020]\n",
      "epoch: 25, [Discriminator :: d_loss: 0.808825], [ Generator :: loss: 0.822720]\n",
      "epoch: 26, [Discriminator :: d_loss: 0.658753], [ Generator :: loss: 0.776148]\n",
      "epoch: 27, [Discriminator :: d_loss: 0.848416], [ Generator :: loss: 0.765929]\n",
      "epoch: 28, [Discriminator :: d_loss: 0.994962], [ Generator :: loss: 0.772095]\n",
      "epoch: 29, [Discriminator :: d_loss: 0.669582], [ Generator :: loss: 0.769654]\n",
      "epoch: 30, [Discriminator :: d_loss: 0.715760], [ Generator :: loss: 0.763586]\n",
      "epoch: 31, [Discriminator :: d_loss: 0.798399], [ Generator :: loss: 0.732288]\n",
      "epoch: 32, [Discriminator :: d_loss: 0.685757], [ Generator :: loss: 0.732796]\n",
      "epoch: 33, [Discriminator :: d_loss: 0.795130], [ Generator :: loss: 0.760988]\n",
      "epoch: 34, [Discriminator :: d_loss: 0.798437], [ Generator :: loss: 0.778839]\n",
      "epoch: 35, [Discriminator :: d_loss: 0.829257], [ Generator :: loss: 0.738988]\n",
      "epoch: 36, [Discriminator :: d_loss: 0.756418], [ Generator :: loss: 0.761172]\n",
      "epoch: 37, [Discriminator :: d_loss: 0.769575], [ Generator :: loss: 0.748984]\n",
      "epoch: 38, [Discriminator :: d_loss: 0.802528], [ Generator :: loss: 0.745729]\n",
      "epoch: 39, [Discriminator :: d_loss: 0.669594], [ Generator :: loss: 0.793887]\n",
      "epoch: 40, [Discriminator :: d_loss: 0.548435], [ Generator :: loss: 0.748544]\n",
      "epoch: 41, [Discriminator :: d_loss: 0.800010], [ Generator :: loss: 0.786129]\n",
      "epoch: 42, [Discriminator :: d_loss: 0.517424], [ Generator :: loss: 0.794399]\n",
      "epoch: 43, [Discriminator :: d_loss: 0.590140], [ Generator :: loss: 0.775024]\n",
      "epoch: 44, [Discriminator :: d_loss: 1.034312], [ Generator :: loss: 0.759549]\n",
      "epoch: 45, [Discriminator :: d_loss: 0.724639], [ Generator :: loss: 0.766944]\n",
      "epoch: 46, [Discriminator :: d_loss: 0.481135], [ Generator :: loss: 0.785860]\n",
      "epoch: 47, [Discriminator :: d_loss: 0.607860], [ Generator :: loss: 0.825365]\n",
      "epoch: 48, [Discriminator :: d_loss: 0.653542], [ Generator :: loss: 0.772608]\n",
      "epoch: 49, [Discriminator :: d_loss: 0.847314], [ Generator :: loss: 0.743515]\n",
      "epoch: 50, [Discriminator :: d_loss: 0.776721], [ Generator :: loss: 0.973631]\n",
      "epoch: 51, [Discriminator :: d_loss: 0.802285], [ Generator :: loss: 0.717547]\n",
      "epoch: 52, [Discriminator :: d_loss: 0.876205], [ Generator :: loss: 0.774758]\n",
      "epoch: 53, [Discriminator :: d_loss: 0.730396], [ Generator :: loss: 0.775309]\n",
      "epoch: 54, [Discriminator :: d_loss: 0.555050], [ Generator :: loss: 0.815141]\n",
      "epoch: 55, [Discriminator :: d_loss: 0.760405], [ Generator :: loss: 0.767271]\n",
      "epoch: 56, [Discriminator :: d_loss: 0.876262], [ Generator :: loss: 0.763273]\n",
      "epoch: 57, [Discriminator :: d_loss: 0.699047], [ Generator :: loss: 0.803485]\n",
      "epoch: 58, [Discriminator :: d_loss: 0.725464], [ Generator :: loss: 0.779363]\n",
      "epoch: 59, [Discriminator :: d_loss: 0.501815], [ Generator :: loss: 0.788897]\n",
      "epoch: 60, [Discriminator :: d_loss: 0.604266], [ Generator :: loss: 0.782881]\n",
      "epoch: 61, [Discriminator :: d_loss: 0.698449], [ Generator :: loss: 0.802761]\n",
      "epoch: 62, [Discriminator :: d_loss: 0.675128], [ Generator :: loss: 0.760796]\n",
      "epoch: 63, [Discriminator :: d_loss: 0.721242], [ Generator :: loss: 0.775652]\n",
      "epoch: 64, [Discriminator :: d_loss: 0.748221], [ Generator :: loss: 0.738635]\n",
      "epoch: 65, [Discriminator :: d_loss: 0.709967], [ Generator :: loss: 0.743684]\n",
      "epoch: 66, [Discriminator :: d_loss: 0.653608], [ Generator :: loss: 0.725182]\n",
      "epoch: 67, [Discriminator :: d_loss: 0.888122], [ Generator :: loss: 0.744570]\n",
      "epoch: 68, [Discriminator :: d_loss: 0.620616], [ Generator :: loss: 0.746400]\n",
      "epoch: 69, [Discriminator :: d_loss: 0.528870], [ Generator :: loss: 0.798841]\n",
      "epoch: 70, [Discriminator :: d_loss: 0.943714], [ Generator :: loss: 0.902686]\n",
      "epoch: 71, [Discriminator :: d_loss: 0.850918], [ Generator :: loss: 0.871591]\n",
      "epoch: 72, [Discriminator :: d_loss: 0.847703], [ Generator :: loss: 0.855000]\n",
      "epoch: 73, [Discriminator :: d_loss: 0.570895], [ Generator :: loss: 0.792647]\n",
      "epoch: 74, [Discriminator :: d_loss: 1.065899], [ Generator :: loss: 0.864834]\n",
      "epoch: 75, [Discriminator :: d_loss: 1.043250], [ Generator :: loss: 0.798081]\n",
      "epoch: 76, [Discriminator :: d_loss: 0.656562], [ Generator :: loss: 0.937356]\n",
      "epoch: 77, [Discriminator :: d_loss: 0.678314], [ Generator :: loss: 0.841677]\n",
      "epoch: 78, [Discriminator :: d_loss: 0.642725], [ Generator :: loss: 0.758497]\n",
      "epoch: 79, [Discriminator :: d_loss: 0.773420], [ Generator :: loss: 0.814328]\n",
      "epoch: 80, [Discriminator :: d_loss: 0.722292], [ Generator :: loss: 0.733180]\n",
      "epoch: 81, [Discriminator :: d_loss: 0.586100], [ Generator :: loss: 0.809910]\n",
      "epoch: 82, [Discriminator :: d_loss: 0.767295], [ Generator :: loss: 0.753957]\n",
      "epoch: 83, [Discriminator :: d_loss: 1.049719], [ Generator :: loss: 1.007443]\n",
      "epoch: 84, [Discriminator :: d_loss: 0.395997], [ Generator :: loss: 0.710154]\n",
      "epoch: 85, [Discriminator :: d_loss: 0.550944], [ Generator :: loss: 0.830995]\n",
      "epoch: 86, [Discriminator :: d_loss: 0.575274], [ Generator :: loss: 0.855910]\n",
      "epoch: 87, [Discriminator :: d_loss: 0.911540], [ Generator :: loss: 0.983195]\n",
      "epoch: 88, [Discriminator :: d_loss: 0.709178], [ Generator :: loss: 0.813588]\n",
      "epoch: 89, [Discriminator :: d_loss: 0.643144], [ Generator :: loss: 0.873466]\n",
      "epoch: 90, [Discriminator :: d_loss: 1.013978], [ Generator :: loss: 0.872171]\n",
      "epoch: 91, [Discriminator :: d_loss: 0.740182], [ Generator :: loss: 0.744069]\n",
      "epoch: 92, [Discriminator :: d_loss: 0.774622], [ Generator :: loss: 0.693553]\n",
      "epoch: 93, [Discriminator :: d_loss: 0.468879], [ Generator :: loss: 0.694949]\n",
      "epoch: 94, [Discriminator :: d_loss: 0.766822], [ Generator :: loss: 0.782201]\n",
      "epoch: 95, [Discriminator :: d_loss: 0.715519], [ Generator :: loss: 0.693333]\n",
      "epoch: 96, [Discriminator :: d_loss: 0.730082], [ Generator :: loss: 0.698224]\n",
      "epoch: 97, [Discriminator :: d_loss: 0.585612], [ Generator :: loss: 0.693289]\n",
      "epoch: 98, [Discriminator :: d_loss: 0.649652], [ Generator :: loss: 0.693147]\n",
      "epoch: 99, [Discriminator :: d_loss: 0.840473], [ Generator :: loss: 0.693147]\n"
     ]
    }
   ],
   "source": [
    "gan = GAN(X.shape[1])\n",
    "gan.train(X, y, debug=True, epochs=100, batch=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preidiction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = gan.discriminator.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.755"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(y == (predict > 0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  2  98]\n",
      " [  0 300]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics \n",
    "print(metrics.confusion_matrix(y, (predict > 0.5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.755"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.accuracy_score(y, (predict > 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=17) \n",
    "cvscores = [] \n",
    "for train, test in kfold.split(X, y): \n",
    "    gan = GAN(X.shape[1]) \n",
    "    gan.train(X[train], y[train], epochs=100, batch=20, debug=False) \n",
    "    predict = gan.discriminator.predict(X[test]) \n",
    "    acc = metrics.f1_score(y[test], (predict > 0.5)) \n",
    "    cvscores.append(acc) \n",
    "    print(acc) \n",
    "print(\"%.2f%% (+/- %.2f%%)\" % (np.mean(cvscores), np.std(cvscores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8595988538681949"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.f1_score(y, (predict > 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
