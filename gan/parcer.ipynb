{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "RAW_DATA_DIR = \"/opt/vanet/data/\"\n",
    "OUTPUT_TABLES_DIR = \"data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys, os, pickle\n",
    "from keras.models import Sequential, model_from_json\n",
    "import pandas as pd\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.optimizers import Adam, RMSprop, SGD\n",
    "import sys, re, glob\n",
    "import numpy as np\n",
    "from keras.layers import Input, Dense, Flatten\n",
    "import sys, re, glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from lxml import etree\n",
    "from sklearn.model_selection import StratifiedKFold \n",
    "from scipy import interp\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import cycle, product\n",
    "from sklearn import metrics \n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import tensorflow as tf\n",
    "from multiprocessing.pool import Pool\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "proto_dict = {\n",
    "    'ns3::WifiMacHeader' : 'MAC',\n",
    "    'ns3::LlcSnapHeader' : 'LLC',\n",
    "    'ns3::ArpHeader' : 'ARP',\n",
    "    'ns3::Ipv4Header' : 'IPv4',\n",
    "    'ns3::UdpHeader' : 'UDP',\n",
    "    'ns3::aodv::TypeHeader' : 'AODV_Type',\n",
    "    'ns3::aodv::RrepHeader' : 'AODV_RREP',\n",
    "    'ns3::aodv::RreqHeader' : 'AODV_RREQ',\n",
    "    'ns3::aodv::RrepAckHeader' : 'AODV_RACK',\n",
    "    'ns3::aodv::RerrHeader' : 'AODV_RERR',\n",
    "    'ns3::Icmpv4Header' : 'ICMPv4_HEADER',\n",
    "    'ns3::Icmpv4TimeExceeded' : 'ICMPv4_TE',\n",
    "    'ns3::Icmpv4DestinationUnreachable' : 'ICMPv4_DU'\n",
    "}\n",
    "\n",
    "def parce_packets(input_file):\n",
    "    with open(input_file, 'r') as f:\n",
    "        data = f.read()\n",
    "    data = data.split('meta-info=\"')\n",
    "    meta_info = []\n",
    "    data.pop(0)\n",
    "    for b in data:\n",
    "        meta_info.append(b.split('\"')[0])\n",
    "    packets = []\n",
    "    for m in meta_info:\n",
    "        raw = {}\n",
    "        for proto in proto_dict:\n",
    "            if m.find(proto) != -1:\n",
    "                raw[proto_dict[proto]] = m.split(proto+' (')[1].split(') ns3')[0]\n",
    "        p = {}\n",
    "        if 'MAC' in raw and 'LLC' in raw:\n",
    "            p['MAC'] = {}\n",
    "            p['MAC']['src'] = raw['MAC'].split('SA=')[1].split(',')[0]\n",
    "            p['MAC']['dst'] = raw['MAC'].split('DA=')[1].split(',')[0]\n",
    "            p['MAC']['type'] = int(raw['LLC'].split('type ')[1], 16)\n",
    "        if 'ARP' in raw:\n",
    "            p['ARP'] = {}\n",
    "            if raw['ARP'].find('request') != -1:\n",
    "                p['ARP']['type'] = 'request'\n",
    "            else:\n",
    "                p['ARP']['type'] = 'reply'\n",
    "                p['ARP']['MAC_dst'] = raw['ARP'].split('dest mac: ')[1].split(' ')[0]\n",
    "            p['ARP']['MAC_src'] = raw['ARP'].split('source mac: ')[1].split(' ')[0]\n",
    "            p['ARP']['IP_src'] = raw['ARP'].split('source ipv4: ')[1].split(' ')[0]\n",
    "            p['ARP']['IP_dst'] = raw['ARP'].split('dest ipv4: ')[1]\n",
    "        if 'IPv4' in raw:\n",
    "            p['IPv4'] = {}\n",
    "            ipv4data = raw['IPv4'].split(' ')\n",
    "            p['IPv4']['src'] = ipv4data[-3]\n",
    "            p['IPv4']['dst'] = ipv4data[-1]\n",
    "            p['IPv4']['proto'] = int(raw['IPv4'].split('protocol ')[1].split(' ')[0])\n",
    "            p['IPv4']['ttl'] = int(raw['IPv4'].split('ttl ')[1].split(' ')[0])\n",
    "            p['IPv4']['length'] = int(raw['IPv4'].split('length: ')[1].split(' ')[0])\n",
    "        if 'UDP' in raw:\n",
    "            p['UDP'] = {}\n",
    "            p['UDP']['length'] = int(raw['UDP'].split('length: ')[1].split(' ')[0])\n",
    "        if 'AODV_Type' in raw:\n",
    "            p['AODV'] = {}\n",
    "            p['AODV']['type'] = raw['AODV_Type']\n",
    "        if 'AODV_RREP' in raw:\n",
    "            p['AODV']['RREP'] = {}\n",
    "            p['AODV']['RREP']['dst'] = raw['AODV_RREP'].split('destination: ipv4 ')[1].split(' ')[0]\n",
    "            p['AODV']['RREP']['src'] = raw['AODV_RREP'].split('source ipv4 ')[1].split(' ')[0]\n",
    "            p['AODV']['RREP']['SN'] = int(raw['AODV_RREP'].split('sequence number ')[1].split(' ')[0])\n",
    "            p['AODV']['RREP']['lifetime'] = int(raw['AODV_RREP'].split('lifetime ')[1].split(' ')[0])\n",
    "            p['AODV']['RREP']['acknowledgment'] = raw['AODV_RREP'].split('acknowledgment ')[1].split(' ')[0]\n",
    "            p['AODV']['RREP']['flag'] = raw['AODV_RREP'].split('flag ')[1]\n",
    "        if 'AODV_RREQ' in raw:\n",
    "            p['AODV']['RREQ'] = {}\n",
    "            p['AODV']['RREQ']['ID'] = int(raw['AODV_RREQ'].split('ID ')[1].split(' ')[0])\n",
    "            p['AODV']['RREQ']['dst'] = raw['AODV_RREQ'].split('destination: ipv4 ')[1].split(' ')[0]\n",
    "            p['AODV']['RREQ']['src'] = raw['AODV_RREQ'].split('source: ipv4 ')[1].split(' ')[0]\n",
    "            p['AODV']['RREQ']['SN'] = int(raw['AODV_RREQ'].split('sequence number ')[1].split(' ')[0])\n",
    "            p['AODV']['RREQ']['flags'] = raw['AODV_RREQ'].split('flags: ')[1]\n",
    "        if 'AODV_RACK' in raw:\n",
    "            pass # seems to be empty\n",
    "        if 'AODV_RERR' in raw:\n",
    "            p['AODV']['RERR'] = raw['AODV_RERR']\n",
    "        if 'ICMPv4_HEADER' in raw:\n",
    "            p['ICMPv4'] = {}\n",
    "            p['ICMPv4']['type'] = int(raw['ICMPv4_HEADER'].split('type=')[1].split(',')[0])\n",
    "            p['ICMPv4']['code'] = int(raw['ICMPv4_HEADER'].split('code=')[1])\n",
    "        if 'ICMPv4_TE' in raw:\n",
    "            p['ICMPv4']['data'] = raw['ICMPv4_TE']\n",
    "        if 'ICMPv4_DU' in raw:\n",
    "            p['ICMPv4']['data'] = raw['ICMPv4_DU']\n",
    "\n",
    "        if len(p) > 0:\n",
    "            packets.append(p)\n",
    "    return packets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mac_to_id(mac):\n",
    "    return int(mac.split(':')[-1], 16) - 1\n",
    "\n",
    "def ip_to_id(ip):\n",
    "    return int(ip.split('.')[-1]) - 1\n",
    "\n",
    "def entropy(arr):\n",
    "    s = arr.sum()\n",
    "    arr = arr / s\n",
    "    arr = arr * np.log(arr) / np.log(2)\n",
    "    return -(arr.sum() / np.log(len(arr)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_scenario(i, path):\n",
    "    routes = list()\n",
    "    flows = list()\n",
    "    packets = list()\n",
    "    \n",
    "    for routes_file in glob.glob('{}/{}_output/*.routes'.format(path, i)):\n",
    "\n",
    "        info = re.findall('(\\w+)/(\\d+)_output', routes_file)\n",
    "\n",
    "        handle = open(routes_file, 'r')\n",
    "        data = handle.read()\n",
    "        handle.close()\n",
    "\n",
    "        nodes = re.split('\\n\\n', data)\n",
    "        nodes.pop()\n",
    "\n",
    "        for node in nodes:\n",
    "\n",
    "            header = re.findall('Node:\\s+(\\d+)\\s+Time:\\s+(\\d+)', node)\n",
    "            lines = re.findall('(\\d{1,3}(?:\\.\\d{1,3}){3})\\s+(\\d{1,3}(?:\\.\\d{1,3}){3})\\s+(\\d{1,3}(?:\\.\\d{1,3}){3})\\s+(\\w+)\\s+(-?\\d+\\.\\d+)\\s+(\\d+)', node)\n",
    "\n",
    "            for line in lines:\n",
    "                l = list(line)\n",
    "                l[4] = float(line[4])\n",
    "                l[5] = int(line[5])\n",
    "                routes.append(header[0] + tuple(l) + info[0])\n",
    "\n",
    "    for flowmon_file in glob.glob('{}/{}_output/*.flowmon'.format(path, i)):\n",
    "\n",
    "        info = re.findall('(\\w+)/(\\d+)_output', flowmon_file)\n",
    "\n",
    "        with open(flowmon_file) as fobj:\n",
    "            xml = fobj.read()\n",
    "\n",
    "        root = etree.fromstring(xml)\n",
    "\n",
    "\n",
    "        for flow in root.xpath('/FlowMonitor/FlowStats/Flow'):\n",
    "\n",
    "            attributes = list()\n",
    "\n",
    "            for attrib in flow.attrib:\n",
    "\n",
    "                attr = flow.attrib[attrib]\n",
    "                if 'ns' in attr:\n",
    "                    attr = re.findall('(\\d+)', attr)[0]\n",
    "\n",
    "                attributes.append(int(attr))\n",
    "\n",
    "            flows.append(tuple(attributes) + info[0])\n",
    "\n",
    "    for packets_file in glob.glob('{}/{}_output/*.xml'.format(path, i)):\n",
    "\n",
    "        if \"routingtable-wireless.xml\" in packets_file:\n",
    "            continue\n",
    "\n",
    "        info = re.findall('(\\w+)/(\\d+)_output', packets_file)\n",
    "\n",
    "        packets_dict = parce_packets(packets_file)\n",
    "\n",
    "        table_packets = []\n",
    "        for p in packets_dict:\n",
    "            if 'IPv4' in p:\n",
    "                table_packets.append([mac_to_id(p['MAC']['src']), mac_to_id(p['MAC']['dst']), ip_to_id(p['IPv4']['src']), ip_to_id(p['IPv4']['dst'])])\n",
    "\n",
    "        table_packets = pd.DataFrame(table_packets, columns=['mac_src', 'mac_dst', 'ip_src', 'ip_dst'])\n",
    "        table_packets = table_packets[table_packets['ip_dst'] != 254]\n",
    "        table_packets['input'] = 0\n",
    "        table_packets['output'] = 0\n",
    "        input_cnt = table_packets.groupby('mac_src').agg({'input':'count'})\n",
    "        output_cnt = table_packets.groupby('mac_dst').agg({'output':'count'})\n",
    "        io_mac = input_cnt.join(output_cnt)\n",
    "        io_mac[\"diff\"] = io_mac['input'] - io_mac['output']\n",
    "        io_mac[\"normalized\"] = io_mac[\"diff\"] - io_mac[\"diff\"].min() + 1\n",
    "\n",
    "        input = io_mac.agg({'input': [entropy, 'min', 'max', 'mean', 'var']}).values.flatten()\n",
    "        output = io_mac.agg({'output': [entropy, 'min', 'max', 'mean', 'var']}).values.flatten()\n",
    "        diff = io_mac.agg({'diff': ['min', 'max', 'mean', 'var']}).values.flatten()\n",
    "        normalized = io_mac.agg({'normalized': [entropy, 'min', 'max', 'mean', 'var']}).values.flatten()\n",
    "        src_count = table_packets[table_packets['mac_src'] == table_packets['ip_src']].count()[0]\n",
    "        dst_count = table_packets[table_packets['mac_dst'] == table_packets['ip_dst']].count()[0]\n",
    "\n",
    "        packets.append(np.concatenate((input, output, diff, normalized, [src_count, dst_count], info[0])))\n",
    "\n",
    "    return routes, flows, packets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path, rng):\n",
    "    \n",
    "    p = Pool()\n",
    "    l = p.map(partial(parse_scenario, path=path), range(rng))\n",
    "    p.close()\n",
    "    routes, flows, packets = tuple(map(lambda x: [j for i in x for j in i],tuple(zip(*l))))\n",
    "\n",
    "    routes_table = pd.DataFrame(routes, columns=['Node', 'Time', 'Destination', 'Gateway', 'Interface', 'Flag', 'Expire', 'Hops', 'Type', 'Test'])\n",
    "    \n",
    "    flag_count = routes_table.groupby(['Type', 'Test', 'Time', 'Flag']).agg({'Flag' : ['count']})\n",
    "    flag_count = flag_count.reset_index(col_level=1)\n",
    "    flag_count.columns = flag_count.columns.droplevel()\n",
    "    flag_agg = flag_count.groupby(['Type', 'Test', 'Flag']).agg({'count': ['max', 'min', 'mean', 'var']}).unstack().reset_index()\n",
    "    flag_agg.columns = [col[0]+col[1]+col[2] for col in flag_agg.columns]\n",
    "    flag_agg = flag_agg.set_index(['Type', 'Test'])\n",
    "\n",
    "    hops_node_dest_agg = routes_table.groupby(['Type', 'Test', 'Node', 'Destination']).agg({'Hops' : ['min', 'max', 'mean']})\n",
    "    hops_node_agg = hops_node_dest_agg.reset_index().groupby([\"Type\", \"Test\", \"Node\"]).agg(['min', 'max', 'mean']).reset_index(col_level=1)\n",
    "    hops_node_agg.columns = hops_node_agg.columns.droplevel()\n",
    "    hops_agg = hops_node_agg.reset_index().groupby([\"Type\", \"Test\"]).agg(['min', 'max', 'mean']).reset_index(col_level=1)\n",
    "    hops_agg.columns = [col[0]+col[1]+col[2] for col in hops_agg.columns]\n",
    "    hops_agg = hops_agg.set_index(['Type', 'Test'])\n",
    "\n",
    "    flows_table = pd.DataFrame(flows, columns=['flowId', 'timeFirstTxPacket', 'timeFirstRxPacket', 'timeLastTxPacket', 'timeLastRxPacket', 'delaySum', 'jitterSum', 'lastDelay', 'txBytes', 'rxBytes', 'txPackets', 'rxPackets', 'lostPackets', 'timesForwarded', 'Type', 'Test'])\n",
    "\n",
    "    lost_agg = flows_table.groupby(['Type', 'Test']).agg({'lostPackets' : ['sum', 'mean']})\n",
    "    forwarded_agg = flows_table.groupby(['Type', 'Test']).agg({'timesForwarded' : ['sum', 'max', 'mean', 'var']})\n",
    "\n",
    "    packets_table = pd.DataFrame(packets, columns=[\"entopy_input\", \"min_input\", \"max_input\", \"mean_input\", \"var_input\", \"entopy_output\", \"min_output\", \"max_output\", \"mean_output\", \"var_output\", \"min_diff\", \"max_diff\", \"mean_diff\", \"var_diff\", \"entopy_normolized\", \"min_normolized\", \"max_normolized\", \"mean_normolized\", \"var_normolized\", \"src_count\", \"dst_count\", 'Type', 'Test'])\n",
    "    packets_table = packets_table.set_index(['Type', 'Test'])\n",
    "\n",
    "    return flag_agg.join(hops_agg).join(lost_agg).join(forwarded_agg).join(packets_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5min 38s, sys: 35.4 s, total: 6min 13s\n",
      "Wall time: 10min 47s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "no_attack = load_data(RAW_DATA_DIR + \"common_no_attack_blackhole/bh_0_1000\", 1000)\n",
    "greyhole_attack = load_data(RAW_DATA_DIR + \"greyhole_attack\", 1000)\n",
    "wormhole_attack = load_data(RAW_DATA_DIR + \"common_wormhole/wormhole_evil2.100\", 1000)\n",
    "blackhole_attack = load_data(RAW_DATA_DIR + \"common_blackhole/bh_0_10000\", 1000)\n",
    "ddos_attack = load_data(RAW_DATA_DIR + \"ddos_attack\", 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_attack.to_csv(OUTPUT_TABLES_DIR + \"no_attack.csv\")\n",
    "greyhole_attack.to_csv(OUTPUT_TABLES_DIR + \"greyhole_attack.csv\")\n",
    "wormhole_attack.to_csv(OUTPUT_TABLES_DIR + \"wormhole_attack.csv\")\n",
    "blackhole_attack.to_csv(OUTPUT_TABLES_DIR + \"blackhole_attack.csv\")\n",
    "ddos_attack.to_csv(OUTPUT_TABLES_DIR + \"ddos_attack.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
